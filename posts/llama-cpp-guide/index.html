<!doctype html><html lang=en><head><title>llama.cpp guide - Running LLMs locally, on any hardware, from scratch :: </title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Psst, kid, want some cheap and small LLMs?"><meta name=keywords content="llama.cpp,llama,cpp,llm,ai,building,running,guide,inference,local,scratch,hardware"><meta name=robots content="noodp"><link rel=canonical href=https://steelph0enix.github.io/posts/llama-cpp-guide/><link rel=stylesheet href=https://steelph0enix.github.io/css/buttons.min.2bc533403a27dfe0e93105a92502b42ce4587e2e4a87d9f7d349e51e16e09478.css><link rel=stylesheet href=https://steelph0enix.github.io/css/code.min.00125962708925857e7b66dbc58391d55be1191a3d0ce2034de8c9cd2c481c36.css><link rel=stylesheet href=https://steelph0enix.github.io/css/fonts.min.4881f0c525f3ce2a1864fb6e96676396cebe1e6fcef1933e8e1dde7041004fb5.css><link rel=stylesheet href=https://steelph0enix.github.io/css/footer.min.2e3eb191baee58dd05a9f0104ac1fab0827bca7c64dafe0b2579f934c33a1d69.css><link rel=stylesheet href=https://steelph0enix.github.io/css/gist.min.a751e8b0abe1ba8bc53ced52a38b19d8950fe78ca29454ea8c2595cf26aad5c0.css><link rel=stylesheet href=https://steelph0enix.github.io/css/header.min.b6fb4423cf82a9f9d7abc9cd010223fa3d70a6526a3f28f8e17d814c06e18f9e.css><link rel=stylesheet href=https://steelph0enix.github.io/css/main.min.fe8dc560fccb53a458b0db19ccb7b265764ac46b68596b7e099c6793054dd457.css><link rel=stylesheet href=https://steelph0enix.github.io/css/menu.min.83637a90d903026bc280d3f82f96ceb06c5fc72b7c1a8d686afb5bbf818a29f7.css><link rel=stylesheet href=https://steelph0enix.github.io/css/pagination.min.82f6400eae7c7c6dc3c866733c2ec0579e4089608fea69400ff85b3880aa0d3c.css><link rel=stylesheet href=https://steelph0enix.github.io/css/post.min.fc74ca360273c1d828da3c02b8174eba435607b369d98418ccc6f2243cd4e75d.css><link rel=stylesheet href=https://steelph0enix.github.io/css/prism.min.9023bbc24533d09e97a51a0a42a5a7bfe4c591ae167c5551fb1d2191d11977c0.css><link rel=stylesheet href=https://steelph0enix.github.io/css/syntax.min.cc789ed9377260d7949ea4c18781fc58959a89287210fe4edbff44ebfc1511b6.css><link rel=stylesheet href=https://steelph0enix.github.io/css/terminal.min.dd0bf9c7cacb24c1b0184f52f1869b274e06689557468cc7030ccf632328eb97.css><link rel=stylesheet href=https://steelph0enix.github.io/css/terms.min.b81791663c3790e738e571cdbf802312390d30e4b1d8dc9d814a5b5454d0ac11.css><link rel=stylesheet href=https://steelph0enix.github.io/terminal.css><link rel=stylesheet href=https://steelph0enix.github.io/style.css><link rel="shortcut icon" href=https://steelph0enix.github.io/favicon.png><link rel=apple-touch-icon href=https://steelph0enix.github.io/apple-touch-icon.png><meta name=twitter:card content="summary"><meta name=twitter:site content><meta name=twitter:creator content="steel_ph0enix"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="og:title" content="llama.cpp guide - Running LLMs locally, on any hardware, from scratch"><meta property="og:description" content="Psst, kid, want some cheap and small LLMs?"><meta property="og:url" content="https://steelph0enix.github.io/posts/llama-cpp-guide/"><meta property="og:site_name" content><meta property="og:image" content="https://steelph0enix.github.io/og-image.png"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="627"><meta property="article:published_time" content="2024-10-28 00:00:00 +0000 UTC"></head><body><div class="container center"><header class=header><div class=header__inner><div class=header__logo><a href=/><div class=logo>SteelPh0enix's Blog</div></a></div><ul class="menu menu--mobile"><li class=menu__trigger>Menu&nbsp;▾</li><li><ul class=menu__dropdown><li><a href=/about>About</a></li></ul></li></ul></div><nav class=navigation-menu><ul class="navigation-menu__inner menu--desktop"><li><a href=/about>About</a></li></ul></nav></header><div class=content><article class=post><h1 class=post-title><a href=https://steelph0enix.github.io/posts/llama-cpp-guide/>llama.cpp guide - Running LLMs locally, on any hardware, from scratch</a></h1><div class=post-meta><time class=post-date>2024-10-28[Updated::2024-11-29]</time><span class=post-author>SteelPh0enix</span><span class=post-reading-time>62 min read (13034 words)</span></div><span class=post-tags>#<a href=https://steelph0enix.github.io/tags/llama.cpp/>llama.cpp</a>&nbsp;
#<a href=https://steelph0enix.github.io/tags/llm/>llm</a>&nbsp;
#<a href=https://steelph0enix.github.io/tags/ai/>ai</a>&nbsp;
#<a href=https://steelph0enix.github.io/tags/guide/>guide</a>&nbsp;</span><div class=table-of-contents><h2>Table of Contents</h2><nav id=TableOfContents><ul><li><a href=#so-i-started-playing-with-llms>so, i started playing with LLMs&mldr;</a></li><li><a href=#but-first---some-disclaimers-for-expectation-management>but first - some disclaimers for expectation management</a><ul><li><a href=#do-i-need-rtx-2070-superrx-7900-xt-ot-similar-midhigh-end-gpu-to-do-what-you-did-here>Do I need RTX 2070 Super/RX 7900 XT ot similar mid/high-end GPU to do what you did here?</a></li><li><a href=#what-performance-can-i-expect>What performance can I expect?</a></li><li><a href=#what-quality-of-responses-can-i-expect>What quality of responses can I expect?</a></li><li><a href=#can-i-replace-chatgptclaudeinsert-online-llm-provider-with-that>Can i replace ChatGPT/Claude/[insert online LLM provider] with that?</a></li></ul></li><li><a href=#prerequisites>prerequisites</a></li><li><a href=#building-the-llama>building the llama</a></li><li><a href=#getting-a-model>getting a model</a><ul><li><a href=#converting-huggingface-model-to-gguf>converting huggingface model to GGUF</a></li><li><a href=#quantizing-the-model>quantizing the model</a></li></ul></li><li><a href=#running-llamacpp-server>running llama.cpp server</a><ul><li><a href=#llamacpp-server-settings>llama.cpp server settings</a></li></ul></li><li><a href=#other-llamacpp-tools>other llama.cpp tools</a><ul><li><a href=#llama-bench><code>llama-bench</code></a></li><li><a href=#llama-cli><code>llama-cli</code></a></li></ul></li><li><a href=#building-the-llama-but-better>building the llama, but better</a></li><li><a href=#llm-configuration-options-explained>LLM configuration options explained</a><ul><li><a href=#how-does-llm-generate-text>how does LLM generate text?</a></li><li><a href=#list-of-llm-configuration-options-and-samplers-available-in-llamacpp>list of LLM configuration options and samplers available in llama.cpp</a></li></ul></li><li><a href=#final-thoughts>final thoughts</a><ul><li><a href=#bonus-where-to-find-models-and-some-recommendations>bonus: where to find models, and some recommendations</a></li></ul></li></ul></nav></div><div class=post-content><div><p><em>No LLMs were harmed during creation of this post.</em></p><h2 id=so-i-started-playing-with-llms>so, i started playing with LLMs&mldr;<a href=#so-i-started-playing-with-llms class=hanchor arialabel=Anchor>#</a></h2><p>&mldr;and it&rsquo;s pretty fun.
I was very skeptical about the AI/LLM &ldquo;boom&rdquo; back when it started.
I thought, like many other people, that they are just mostly making stuff up, and generating uncanny-valley-tier nonsense.
Boy, was i wrong.
I&rsquo;ve used ChatGPT once or twice, to test the waters - it made a pretty good first impression, despite hallucinating a bit.
That was back when GPT3.5 was the top model. We came a pretty long way since then.</p><p>However, despite ChatGPT not disappointing me, i was still skeptical.
Everything i&rsquo;ve wrote, and every piece of response was fully available to OpenAI, or whatever other provider i&rsquo;d want to use.
This is not a big deal, but it tickles me in a wrong way, and also means i can&rsquo;t use LLMs for any work-related non-open-source stuff.
Also, ChatGPT is free only to a some degree - if i&rsquo;d want to go full-in on AI, i&rsquo;d probably have to start paying.
Which, obviously, i&rsquo;d rather avoid.</p><p>At some point i started looking at open-source models.
I had no idea how to use them, but the moment i saw the sizes of &ldquo;small&rdquo; models, like Llama 2 7B, i&rsquo;ve realized that my RTX 2070 Super with mere 8GB of VRAM would probably have issues running them (i was wrong on that too!), and running them on CPU would probably yield very bad performance.
And then, i&rsquo;ve bought a new GPU - RX 7900 XT, with 20GB of VRAM, which is definitely more than enough to run small-to-medium LLMs.
Yay!</p><p>Now my issue was finding some software that could run an LLM on that GPU.
CUDA was the most popular back-end - but that&rsquo;s for NVidia GPUs, not AMD.
After doing a bit of research, i&rsquo;ve found out about ROCm and found <a href=https://lmstudio.ai/>LM Studio</a>.
And this was exactly what i was looking for - at least for the time being.
Great UI, easy access to many models, and the quantization - that was the thing that absolutely sold me into self-hosting LLMs.
Existence of quantization made me realize that you don&rsquo;t need powerful hardware for running LLMs!
You can even run <a href=https://www.reddit.com/r/raspberry_pi/comments/1ati2ki/how_to_run_a_large_language_model_llm_on_a/>LLMs on RaspberryPi&rsquo;s</a> at this point (with <code>llama.cpp</code> too!)
Of course, the performance will be <em>abysmal</em> if you don&rsquo;t run the LLM with a proper backend on a decent hardware, but the bar is currently not very high.</p><p>If you came here with intention of finding some piece of software that will allow you to <strong>easily run popular models on most modern hardware for non-commercial purposes</strong> - grab <a href=https://lmstudio.ai/>LM Studio</a>, read the <a href=/posts/llama-cpp-guide/#but-first---some-disclaimers-for-expectation-management>next section</a> of this post, and go play with it.
It fits this description very well, just make sure to use appropriate back-end for your GPU/CPU for optimal performance.</p><p>However, if you:</p><ul><li>Want to learn more about <code>llama.cpp</code> (which LM Studio uses as a back-end), and LLMs in general</li><li>Want to use LLMs for commercial purposes (<a href=https://lmstudio.ai/terms>LM Studio&rsquo;s terms</a> forbid that)</li><li>Want to run LLMs on exotic hardware (LM Studio provides only the most popular backends)</li><li>Don&rsquo;t like closed-source software (which LM Studio, unfortunately, is) and/or don&rsquo;t trust anything you don&rsquo;t build yourself</li><li>Want to have access to latest features and models as soon as possible</li></ul><p>you should find the rest of this post pretty useful!</p><h2 id=but-first---some-disclaimers-for-expectation-management>but first - some disclaimers for expectation management<a href=#but-first---some-disclaimers-for-expectation-management class=hanchor arialabel=Anchor>#</a></h2><p>Before i proceed, i want to make some stuff clear.
This &ldquo;FAQ&rdquo; answers some questions i&rsquo;d like to know answers to before getting into self-hosted LLMs.</p><h3 id=do-i-need-rtx-2070-superrx-7900-xt-ot-similar-midhigh-end-gpu-to-do-what-you-did-here>Do I need RTX 2070 Super/RX 7900 XT ot similar mid/high-end GPU to do what you did here?<a href=#do-i-need-rtx-2070-superrx-7900-xt-ot-similar-midhigh-end-gpu-to-do-what-you-did-here class=hanchor arialabel=Anchor>#</a></h3><p>No, you don&rsquo;t.
I&rsquo;ll elaborate later, but you can run LLMs with no GPU at all.
As long as you have reasonably modern hardware (by that i mean <em>at least</em> a decent CPU with AVX support) - you&rsquo;re <em>compatible</em>.
<strong>But remember - your performance may vary.</strong></p><h3 id=what-performance-can-i-expect>What performance can I expect?<a href=#what-performance-can-i-expect class=hanchor arialabel=Anchor>#</a></h3><p>This is a very hard question to answer directly.
The speed of text generation depends on multiple factors, but primarily</p><ul><li>matrix operations performance on your hardware</li><li>memory bandwidth</li><li>model size</li></ul><p>I&rsquo;ll explain this with more details later, but you can generally get reasonable performance from the LLM by picking model small enough for your hardware.
If you intend to use GPU, and it has enough memory for a model with it&rsquo;s context - expect real-time text generation.
In case you want to use both GPU and CPU, or only CPU - you should expect much lower performance, but real-time text generation is possible with small models.</p><h3 id=what-quality-of-responses-can-i-expect>What quality of responses can I expect?<a href=#what-quality-of-responses-can-i-expect class=hanchor arialabel=Anchor>#</a></h3><p>That heavily depends on your usage and chosen model.
I can&rsquo;t answer that question directly, you&rsquo;ll have to play around and find out yourself.
A rule of thumb is &ldquo;larger the model, better the response&rdquo; - consider the fact that size of SOTA (state-of-the-art) models, like GPT-4 or Claude, is usually measured in hundreds of billions of parameters.
Unless you have multiple GPUs or unreasonable amount of RAM and patience - you&rsquo;ll most likely be restricted to models with less than 20 billion parameters.
From my experience, 7-8B models are pretty good for generic purposes and programming - and they are not <em>very</em> far from SOTA models like GPT-4o or Claude in terms of raw quality of generated responses, but the difference is definitely noticeable.
Keep in mind that the choice of a model is only a part of the problem - providing proper context and system prompt, or fine-tuning LLMs can do wonders.</p><h3 id=can-i-replace-chatgptclaudeinsert-online-llm-provider-with-that>Can i replace ChatGPT/Claude/[insert online LLM provider] with that?<a href=#can-i-replace-chatgptclaudeinsert-online-llm-provider-with-that class=hanchor arialabel=Anchor>#</a></h3><p>Maybe. In theory - yes, but in practice - it depends on your tools.
<code>llama.cpp</code> provides OpenAI-compatible server.
As long as your tools communicate with LLMs via OpenAI API, and you are able to set custom endpoint, you will be able to use self-hosted LLM with them.</p><h2 id=prerequisites>prerequisites<a href=#prerequisites class=hanchor arialabel=Anchor>#</a></h2><ul><li>Reasonably modern CPU.
If you&rsquo;re rocking any Ryzen, or Intel&rsquo;s 8th gen or newer, you&rsquo;re good to go, but all of this should work on older hardware too.</li><li>Optimally, a GPU.
More VRAM, the better.
If you have at least 8GB of VRAM, you should be able to run 7-8B models, i&rsquo;d say that it&rsquo;s reasonable minimum.
Vendor doesn&rsquo;t matter, llama.cpp supports NVidia, AMD and Apple GPUs (not sure about Intel, but i think i saw a backend for that - if not, Vulkan should work).</li><li>If you&rsquo;re not using GPU or it doesn&rsquo;t have enough VRAM, you need RAM for the model.
As above, at least 8GB of free RAM is recommended, but more is better.
Keep in mind that when only GPU is used by llama.cpp, RAM usage is very low.</li></ul><p>In the guide, i&rsquo;ll assume you&rsquo;re using either Windows or Linux.
I can&rsquo;t provide any support for Mac users, so they should follow Linux steps and consult the llama.cpp docs wherever possible.</p><p>Some context-specific formatting is used in this post:</p><blockquote><p class=windows-bg>Parts of this post where i&rsquo;ll write about Windows-specific stuff will have this background.
You&rsquo;ll notice they&rsquo;re much longer than Linux ones - Windows is a PITA.
Linux is preferred. I will still explain everything step-by-step for Windows, but in case of issues - try Linux.</p></blockquote><blockquote><p class=linux-bg>And parts where i&rsquo;ll write about Linux-specific stuff will have this background.</p></blockquote><h2 id=building-the-llama>building the llama<a href=#building-the-llama class=hanchor arialabel=Anchor>#</a></h2><p>In <a href=https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md><code>docs/build.md</code></a>, you&rsquo;ll find detailed build instructions for all the supported platforms.
By default, <code>llama.cpp</code> builds with auto-detected CPU support.
We&rsquo;ll talk about enabling GPU support later, first - let&rsquo;s try building it as-is, because it&rsquo;s a good baseline to start with, and it doesn&rsquo;t require any external dependencies.
To do that, we only need a C++ toolchain, <a href=https://cmake.org/>CMake</a> and <a href=https://ninja-build.org/>Ninja</a>.</p><blockquote><p>If you are <strong>very</strong> lazy, you can download a release from Github and skip building steps.
Make sure to download correct version for your hardware/backend.
If you have troubles picking, i recommend following the build guide anyway - it&rsquo;s simple enough and should explain what you should be looking for.
Keep in mind that release won&rsquo;t contain Python scripts that we&rsquo;re going to use, so if you&rsquo;ll want to quantize models manually, you&rsquo;ll need to get them from repository.</p></blockquote><blockquote><p>On Windows, i recommend using <a href=https://www.msys2.org/>MSYS</a> to setup the environment for building and using <code>llama.cpp</code>.
<a href=https://visualstudio.microsoft.com/downloads/>Microsoft Visual C++</a> is supported too, but trust me on that - you&rsquo;ll want to use MSYS instead (it&rsquo;s still a bit of pain in the ass, Linux setup is much simpler).
Follow the guide on the main page to install MinGW for x64 UCRT environment, which you probably should be using.
CMake, Ninja and Git can be installed in UCRT MSYS environment like that:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>pacman -S git mingw-w64-ucrt-x86_64-cmake mingw-w64-ucrt-x86_64-ninja
</span></span></code></pre></div><p>However, if you&rsquo;re using any other toolchain (MSVC, or non-MSYS one), you should install CMake, Git and Ninja via <code>winget</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=display:flex><span>winget install cmake git.git ninja-build.ninja
</span></span></code></pre></div><p>You&rsquo;ll also need Python, which you can get via winget.
Get the latest available version, at the time of writing this post it&rsquo;s 3.13.</p><p><strong>DO NOT USE PYTHON FROM MSYS, IT WILL NOT WORK PROPERLY DUE TO ISSUES WITH BUILDING <code>llama.cpp</code> DEPENDENCY PACKAGES!</strong>
<strong>We&rsquo;re going to be using MSYS only for <em>building</em> <code>llama.cpp</code>, nothing more.</strong></p><p><strong>If you&rsquo;re using MSYS, remember to add it&rsquo;s <code>/bin</code> (<code>C:\msys64\ucrt64\bin</code> by default) directory to PATH, so Python can use MinGW for building packages.</strong>
<strong>Check if GCC is available by opening PowerShell/Command line and trying to run <code>gcc --version</code>.</strong>
Also; check if it&rsquo;s <em>correct</em> GCC by running <code>where.exe gcc.exe</code> and seeing where the first entry points to.
Reorder your PATH if you&rsquo;ll notice that you&rsquo;re using wrong GCC.</p><p><strong>If you&rsquo;re using MSVC - ignore this disclaimer, it should be &ldquo;detectable&rdquo; by default.</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=display:flex><span>winget install python.python.3.13
</span></span></code></pre></div><p>I recommend installing/upgrading <code>pip</code>, <code>setuptools</code> and <code>wheel</code> packages before continuing.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=display:flex><span>python -m pip install --upgrade pip wheel setuptools
</span></span></code></pre></div></blockquote><blockquote><p class=linux-bg>On Linux, GCC is recommended, but you should be able to use Clang if you&rsquo;d prefer by setting <code>CMAKE_C_COMPILER=clang</code> and <code>CMAKE_CXX_COMPILER=clang++</code> variables.
You should have GCC preinstalled (check <code>gcc --version</code> in terminal), if not - get latest version for your distribution using your package manager.
Same applies to CMake, Ninja, Python 3 (with <code>setuptools</code>, <code>wheel</code> and <code>pip</code>) and Git.</p></blockquote><p>Let&rsquo;s start by grabbing a copy of <a href=https://github.com/ggerganov/llama.cpp><code>llama.cpp</code> source code</a>, and moving into it.</p><blockquote><p>Disclaimer: this guide assumes all commands are ran from user&rsquo;s home directory (<code>/home/[yourusername]</code> on Linux, <code>C:/Users/[yourusername]</code> on Windows).
You can use any directory you&rsquo;d like, just keep in mind that if &ldquo;starting directory&rdquo; is not explicitly mentioned, start from home dir/your chosen one.</p></blockquote><blockquote><p class=windows-bg><strong>If you&rsquo;re using MSYS</strong>, remember that MSYS home directory is different from Windows home directory. Make sure to use <code>cd</code> (without arguments) to move into it after starting MSYS.</p></blockquote><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>git clone git@github.com:ggerganov/llama.cpp.git
</span></span><span style=display:flex><span>cd llama.cpp
</span></span><span style=display:flex><span>git submodule update --init --recursive
</span></span></code></pre></div><p>Now we&rsquo;ll use CMake to generate build files, build the project, and install it.
Run the following command to generate build files in <code>build/</code> subdirectory:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>cmake -S . -B build -G Ninja -DCMAKE_BUILD_TYPE<span style=color:#f92672>=</span>Release -DCMAKE_INSTALL_PREFIX<span style=color:#f92672>=</span>/your/install/dir -DLLAMA_BUILD_TESTS<span style=color:#f92672>=</span>OFF -DLLAMA_BUILD_EXAMPLES<span style=color:#f92672>=</span>ON -DLLAMA_BUILD_SERVER<span style=color:#f92672>=</span>ON
</span></span></code></pre></div><p>There&rsquo;s a lot of CMake variables being defined, which we could ignore and let llama.cpp use it&rsquo;s defaults, but we won&rsquo;t:</p><ul><li><code>CMAKE_BUILD_TYPE</code> is set to release for obvious reasons - we want maximum performance.</li><li><a href=https://cmake.org/cmake/help/latest/variable/CMAKE_INSTALL_PREFIX.html><code>CMAKE_INSTALL_PREFIX</code></a> is where the <code>llama.cpp</code> binaries and python scripts will go. Replace the value of this variable, or remove it&rsquo;s definition to keep default value.<ul><li>On Windows, default directory is <code>c:/Program Files/llama.cpp</code>.
As above, you&rsquo;ll need admin privileges to install it, and you&rsquo;ll have to add the <code>bin/</code> subdirectory to your <code>PATH</code> to make llama.cpp binaries accessible system-wide.
I prefer installing llama.cpp in <code>$env:LOCALAPPDATA/llama.cpp</code> (<code>C:/Users/[yourusername]/AppData/Local/llama.cpp</code>), as it doesn&rsquo;t require admin privileges.</li><li>On Linux, default directory is <code>/usr/local</code>.
You can ignore this variable if that&rsquo;s fine with you, but you&rsquo;ll need superuser permissions to install the binaries there.
If you don&rsquo;t have them, change it to point somewhere in your user directory and add it&rsquo;s <code>bin/</code> subdirectory to <code>PATH</code>.</li></ul></li><li><code>LLAMA_BUILD_TESTS</code> is set to <code>OFF</code> because we don&rsquo;t need tests, it&rsquo;ll make the build a bit quicker.</li><li><code>LLAMA_BUILD_EXAMPLES</code> is <code>ON</code> because we&rsquo;re gonna be using them.</li><li><code>LLAMA_BUILD_SERVER</code> - see above. Note: Disabling <code>LLAMA_BUILD_EXAMPLES</code> unconditionally disables building the server, both must be <code>ON</code>.</li></ul><p>Now, let&rsquo;s build the project.
Replace <code>X</code> with amount of cores your CPU has for faster compilation.
In theory, Ninja should automatically use all available cores, but i still prefer passing this argument manually.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>cmake --build build --config Release -j X
</span></span></code></pre></div><p>Building should take only a few minutes.
After that, we can install the binaries for easier usage.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>cmake --install build --config Release
</span></span></code></pre></div><p>Now, after going to <code>CMAKE_INSTALL_PREFIX/bin</code> directory, we should see a list of executables and Python scripts:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>/c/Users/phoen/llama-build/bin
</span></span><span style=display:flex><span>❯ l
</span></span><span style=display:flex><span>Mode  Size Date Modified Name
</span></span><span style=display:flex><span>-a--- 203k  7 Nov 16:14  convert_hf_to_gguf.py
</span></span><span style=display:flex><span>-a--- 3.9M  7 Nov 16:18  llama-batched-bench.exe
</span></span><span style=display:flex><span>-a--- 3.9M  7 Nov 16:18  llama-batched.exe
</span></span><span style=display:flex><span>-a--- 3.4M  7 Nov 16:18  llama-bench.exe
</span></span><span style=display:flex><span>-a--- 3.9M  7 Nov 16:18  llama-cli.exe
</span></span><span style=display:flex><span>-a--- 3.2M  7 Nov 16:18  llama-convert-llama2c-to-ggml.exe
</span></span><span style=display:flex><span>-a--- 3.9M  7 Nov 16:18  llama-cvector-generator.exe
</span></span><span style=display:flex><span>-a--- 3.9M  7 Nov 16:18  llama-embedding.exe
</span></span><span style=display:flex><span>-a--- 3.9M  7 Nov 16:18  llama-eval-callback.exe
</span></span><span style=display:flex><span>-a--- 3.9M  7 Nov 16:18  llama-export-lora.exe
</span></span><span style=display:flex><span>-a--- 3.0M  7 Nov 16:18  llama-gbnf-validator.exe
</span></span><span style=display:flex><span>-a--- 1.2M  7 Nov 16:18  llama-gguf-hash.exe
</span></span><span style=display:flex><span>-a--- 3.0M  7 Nov 16:18  llama-gguf-split.exe
</span></span><span style=display:flex><span>-a--- 1.1M  7 Nov 16:18  llama-gguf.exe
</span></span><span style=display:flex><span>-a--- 3.9M  7 Nov 16:18  llama-gritlm.exe
</span></span><span style=display:flex><span>-a--- 3.9M  7 Nov 16:18  llama-imatrix.exe
</span></span><span style=display:flex><span>-a--- 3.9M  7 Nov 16:18  llama-infill.exe
</span></span><span style=display:flex><span>-a--- 4.2M  7 Nov 16:18  llama-llava-cli.exe
</span></span><span style=display:flex><span>-a--- 3.9M  7 Nov 16:18  llama-lookahead.exe
</span></span><span style=display:flex><span>-a--- 3.9M  7 Nov 16:18  llama-lookup-create.exe
</span></span><span style=display:flex><span>-a--- 1.2M  7 Nov 16:18  llama-lookup-merge.exe
</span></span><span style=display:flex><span>-a--- 3.9M  7 Nov 16:18  llama-lookup-stats.exe
</span></span><span style=display:flex><span>-a--- 3.9M  7 Nov 16:18  llama-lookup.exe
</span></span><span style=display:flex><span>-a--- 4.1M  7 Nov 16:18  llama-minicpmv-cli.exe
</span></span><span style=display:flex><span>-a--- 3.9M  7 Nov 16:18  llama-parallel.exe
</span></span><span style=display:flex><span>-a--- 3.9M  7 Nov 16:18  llama-passkey.exe
</span></span><span style=display:flex><span>-a--- 4.0M  7 Nov 16:18  llama-perplexity.exe
</span></span><span style=display:flex><span>-a--- 3.0M  7 Nov 16:18  llama-quantize-stats.exe
</span></span><span style=display:flex><span>-a--- 3.2M  7 Nov 16:18  llama-quantize.exe
</span></span><span style=display:flex><span>-a--- 3.9M  7 Nov 16:18  llama-retrieval.exe
</span></span><span style=display:flex><span>-a--- 3.9M  7 Nov 16:18  llama-save-load-state.exe
</span></span><span style=display:flex><span>-a--- 5.0M  7 Nov 16:19  llama-server.exe
</span></span><span style=display:flex><span>-a--- 3.0M  7 Nov 16:18  llama-simple-chat.exe
</span></span><span style=display:flex><span>-a--- 3.0M  7 Nov 16:18  llama-simple.exe
</span></span><span style=display:flex><span>-a--- 3.9M  7 Nov 16:18  llama-speculative.exe
</span></span><span style=display:flex><span>-a--- 3.1M  7 Nov 16:18  llama-tokenize.exe
</span></span></code></pre></div><p>Don&rsquo;t feel overwhelmed by the amount, we&rsquo;re only going to be using few of them.
You should try running one of them to check if the executables have built correctly, for example - try <code>llama-cli --help</code>.
We can&rsquo;t do anything meaningful yet, because we lack a single critical component - a model to run.</p><h2 id=getting-a-model>getting a model<a href=#getting-a-model class=hanchor arialabel=Anchor>#</a></h2><p>The main place to look for models is <a href=https://huggingface.co/>HuggingFace</a>.
You can also find datasets and other AI-related stuff there, great site.</p><p>We&rsquo;re going to use <a href=https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9><code>SmolLM2</code></a> here, a model series created by HuggingFace and published fairly recently (1st November 2024).
The reason i&rsquo;ve chosen this model is the size - as the name implies, it&rsquo;s <em>small</em>.
Largest model from this series has 1.7 billion parameters, which means that it requires approx. 4GB of system memory to run in <em>raw, unquantized</em> form (excluding context)!
There are also 360M and 135M variants, which are even smaller and should be easily runnable on RaspberryPi or a smartphone.</p><p>There&rsquo;s but one issue - <code>llama.cpp</code> cannot run &ldquo;raw&rdquo; models directly.
What is usually provided by most LLM creators are original weights in <code>.safetensors</code> or similar format.
<code>llama.cpp</code> expects models in <code>.gguf</code> format.
Fortunately, there is a very simple way of converting original model weights into <code>.gguf</code> - <code>llama.cpp</code> provides <code>convert_hf_to_gguf.py</code> script exactly for this purpose!
Sometimes the creator provides <code>.gguf</code> files - for example, two variants of <code>SmolLM2</code> are provided by HuggingFace in this format.
This is not a very common practice, but you can also find models in <code>.gguf</code> format uploaded there by community.
However, i&rsquo;ll ignore the existence of pre-quantized <code>.gguf</code> files here, and focus on quantizing our models by ourselves here, as it&rsquo;ll allow us to experiment and adjust the quantization parameters of our model without having to download it multiple times.</p><p>Grab the content of <a href=https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct/tree/main>SmolLM2 1.7B Instruct</a> repository (you can use <a href=https://huggingface.co/HuggingFaceTB/SmolLM2-360M-Instruct/tree/main>360M Instruct</a> or <a href=https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct/tree/main>135M Instruct</a> version instead, if you have less than 4GB of free (V)RAM - or use any other model you&rsquo;re already familiar with, if it supports <code>transformers</code>), but omit the LFS files - we only need a single one, and we&rsquo;ll download it manually.</p><blockquote><p>Why <em>Instruct</em>, specifically?
You might have noticed that there are two variants of all those models - <em>Instruct</em> and <em>the other one without a suffix</em>.
<em>Instruct</em> is trained for chat conversations, base model is only trained for text completion and is usually used as a base for further training.
This rule applies to most LLMs, but not all, so make sure to read the model&rsquo;s description before using it!</p></blockquote><p class=linux-bg-padded>If you&rsquo;re using Bash/ZSH or compatible shell:</p><p class=windows-bg-padded><em>MSYS uses Bash by default, so it applies to it too.</em>
<em>From now on, assume that Linux commands work on MSYS too, unless i explicitly say otherwise.</em></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>GIT_LFS_SKIP_SMUDGE<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span> git clone https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct
</span></span></code></pre></div><p class=windows-bg-padded>If you&rsquo;re using PowerShell:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$env:GIT_LFS_SKIP_SMUDGE<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>git clone https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct
</span></span></code></pre></div><p class=windows-bg-padded>If you&rsquo;re using cmd.exe (VS Development Prompt, for example):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>set GIT_LFS_SKIP_SMUDGE<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>git clone https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct
</span></span></code></pre></div><p>HuggingFace also supports Git over SSH. You can look up the <code>git clone</code> command for every repo here:
<img alt="huggingface - where to find git clone button" src=/img/llama-cpp/clone-hf-button.png></p><p>After cloning the repo, <strong>download the <code>model.safetensors</code> file from HuggingFace manually.</strong>
The reason why we used <code>GIT_LFS_SKIP_SMUDGE</code> is because there&rsquo;s many other large model files hidden in repo, and we don&rsquo;t need them.
Also, downloading very large files manually is faster, because Git LFS sucks in that regard.</p><p>After downloading everything, our local copy of the SmolLM repo should look like this:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>PS D:\LLMs\repos\SmolLM2-1.7B-Instruct&gt; l
</span></span><span style=display:flex><span>Mode  Size Date Modified Name
</span></span><span style=display:flex><span>-a---  806  2 Nov 15:16  all_results.json
</span></span><span style=display:flex><span>-a---  888  2 Nov 15:16  config.json
</span></span><span style=display:flex><span>-a---  602  2 Nov 15:16  eval_results.json
</span></span><span style=display:flex><span>-a---  139  2 Nov 15:16  generation_config.json
</span></span><span style=display:flex><span>-a--- 515k  2 Nov 15:16  merges.txt
</span></span><span style=display:flex><span>-a--- 3.4G  2 Nov 15:34  model.safetensors
</span></span><span style=display:flex><span>d----    -  2 Nov 15:16  onnx
</span></span><span style=display:flex><span>-a---  11k  2 Nov 15:16  README.md
</span></span><span style=display:flex><span>d----    -  2 Nov 15:16  runs
</span></span><span style=display:flex><span>-a---  689  2 Nov 15:16  special_tokens_map.json
</span></span><span style=display:flex><span>-a--- 2.2M  2 Nov 15:16  tokenizer.json
</span></span><span style=display:flex><span>-a--- 3.9k  2 Nov 15:16  tokenizer_config.json
</span></span><span style=display:flex><span>-a---  240  2 Nov 15:16  train_results.json
</span></span><span style=display:flex><span>-a---  89k  2 Nov 15:16  trainer_state.json
</span></span><span style=display:flex><span>-a---  129  2 Nov 15:16  training_args.bin
</span></span><span style=display:flex><span>-a--- 801k  2 Nov 15:16  vocab.json
</span></span></code></pre></div><p>We&rsquo;re gonna (indirectly) use only four of those files:</p><ul><li><code>config.json</code> contains configuration/metadata of our model</li><li><code>model.safetensors</code> contains model weights</li><li><code>tokenizer.json</code> contains tokenizer data (mapping of text tokens to their ID&rsquo;s, and other stuff).
Sometimes this data is stored in <code>tokenizer.model</code> file instead.</li><li><code>tokenizer_config.json</code> contains tokenizer configuration (for example, special tokens and chat template)</li></ul><p class=anti-plag>i&rsquo;m leaving this sentence here as anti-plagiarism token.
If you&rsquo;re not currently reading this on my blog, which is @ steelph0enix.github.io, someone probably stolen that article without permission</p><h3 class=post-anti-plag id=converting-huggingface-model-to-gguf>converting huggingface model to GGUF</h3><p>In order to convert this raw model to something that <code>llama.cpp</code> will understand, we&rsquo;ll use aforementioned <code>convert_hf_to_gguf.py</code> script that comes with <code>llama.cpp</code>.
For all our Python needs, we&rsquo;re gonna need a virtual environment.
I recommend making it outside of <code>llama.cpp</code> repo, for example - in your home directory.</p><p class=linux-bg-padded>To make one on Linux, run this command (tweak the path if you&rsquo;d like):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>python -m venv ~/llama-cpp-venv
</span></span></code></pre></div><p class=windows-bg-padded>If you&rsquo;re using PowerShell, this is the equivalent:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=display:flex><span>python -m venv $env:USERPROFILE/llama-cpp-venv
</span></span></code></pre></div><p class=windows-bg-padded>If you&rsquo;re using cmd.exe, this is the equivalent:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-batch data-lang=batch><span style=display:flex><span>python -m venv %USERPROFILE%/llama-cpp-venv
</span></span></code></pre></div><p>Then, we need to activate it.</p><p class=linux-bg-padded>On Linux:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>source ~/llama-cpp-venv/bin/activate
</span></span></code></pre></div><p class=windows-bg-padded>With PowerShell:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=display:flex><span>. $env:USERPROFILE/llama-cpp-venv/Scripts/Activate.ps1
</span></span></code></pre></div><p class=windows-bg-padded>With cmd.exe:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cmd data-lang=cmd><span style=display:flex><span><span style=color:#66d9ef>call</span> %USERPROFILE%/llama-cpp-venv/Scripts/activate.bat
</span></span></code></pre></div><p>After that, let&rsquo;s make sure that our virtualenv has all the core packages up-to-date.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>python -m pip install --upgrade pip wheel setuptools
</span></span></code></pre></div><p>Next, we need to install prerequisites for the llama.cpp scripts.
Let&rsquo;s look into <code>requirements/</code> directory of our <code>llama.cpp</code> repository.
We should see something like this:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>❯ l llama.cpp/requirements
</span></span><span style=display:flex><span>Mode  Size Date Modified Name
</span></span><span style=display:flex><span>-a---  428 11 Nov 13:57  requirements-all.txt
</span></span><span style=display:flex><span>-a---   34 11 Nov 13:57  requirements-compare-llama-bench.txt
</span></span><span style=display:flex><span>-a---  111 11 Nov 13:57  requirements-convert_hf_to_gguf.txt
</span></span><span style=display:flex><span>-a---  111 11 Nov 13:57  requirements-convert_hf_to_gguf_update.txt
</span></span><span style=display:flex><span>-a---   99 11 Nov 13:57  requirements-convert_legacy_llama.txt
</span></span><span style=display:flex><span>-a---   43 11 Nov 13:57  requirements-convert_llama_ggml_to_gguf.txt
</span></span><span style=display:flex><span>-a---   96 11 Nov 13:57  requirements-convert_lora_to_gguf.txt
</span></span><span style=display:flex><span>-a---   48 11 Nov 13:57  requirements-pydantic.txt
</span></span><span style=display:flex><span>-a---   13 11 Nov 13:57  requirements-test-tokenizer-random.txt
</span></span></code></pre></div><p>As we can see, there&rsquo;s a file with deps for our script!
To install dependencies from it, run this command:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>python -m pip install --upgrade -r llama.cpp/requirements/requirements-convert_hf_to_gguf.txt
</span></span></code></pre></div><p>If <code>pip</code> failed during build, make sure you have working C/C++ toolchain in your <code>PATH</code>.</p><blockquote><p class=windows-bg>If you&rsquo;re using MSYS for that, don&rsquo;t. Go back to PowerShell/cmd, install Python via winget and repeat the setup.
As far as i&rsquo;ve tested it, Python deps don&rsquo;t detect the platform correctly on MSYS and try to use wrong build config.
This is what i warned you about earlier.</p></blockquote><p>Now we can use the script to create our GGUF model file.
Start with printing the help and reading the options.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>python llama.cpp/convert_hf_to_gguf.py --help
</span></span></code></pre></div><p>If this command printed help, you can continue.
Otherwise make sure that python&rsquo;s virtualenv is active and dependencies are correctly installed, and try again.
To convert our model we can simply pass the path to directory with model&rsquo;s repository and, optionally, path to output file.
We don&rsquo;t need to tweak the quantization here, for maximum flexibility we&rsquo;re going to create a floating-point GGUF file which we&rsquo;ll then quantize down.
That&rsquo;s because <code>llama-quantize</code> offers much more quantization options, and this script picks optimal floating-point format by default.</p><p>To create GGUF file from our downloaded HuggingFace repository with SmolLM2 (Replace <code>SmolLM2-1.7B-Instruct</code> with your path, if it&rsquo;s different) run this command:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>python llama.cpp/convert_hf_to_gguf.py SmolLM2-1.7B-Instruct --outfile ./SmolLM2.gguf
</span></span></code></pre></div><p>If everything went correctly, you should see similar output:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>INFO:hf-to-gguf:Loading model: SmolLM2-1.7B-Instruct
</span></span><span style=display:flex><span>INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only
</span></span><span style=display:flex><span>INFO:hf-to-gguf:Exporting model...
</span></span><span style=display:flex><span>INFO:hf-to-gguf:gguf: loading model part &#39;model.safetensors&#39;
</span></span><span style=display:flex><span>INFO:hf-to-gguf:token_embd.weight,           torch.bfloat16 --&gt; F16, shape = {2048, 49152}
</span></span><span style=display:flex><span>INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {2048}
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.bfloat16 --&gt; F16, shape = {2048, 2048}
</span></span><span style=display:flex><span>INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.bfloat16 --&gt; F16, shape = {2048, 2048}
</span></span><span style=display:flex><span>INFO:hf-to-gguf:output_norm.weight,          torch.bfloat16 --&gt; F32, shape = {2048}
</span></span><span style=display:flex><span>INFO:hf-to-gguf:Set meta model
</span></span><span style=display:flex><span>INFO:hf-to-gguf:Set model parameters
</span></span><span style=display:flex><span>INFO:hf-to-gguf:gguf: context length = 8192
</span></span><span style=display:flex><span>INFO:hf-to-gguf:gguf: embedding length = 2048
</span></span><span style=display:flex><span>INFO:hf-to-gguf:gguf: feed forward length = 8192
</span></span><span style=display:flex><span>INFO:hf-to-gguf:gguf: head count = 32
</span></span><span style=display:flex><span>INFO:hf-to-gguf:gguf: key-value head count = 32
</span></span><span style=display:flex><span>INFO:hf-to-gguf:gguf: rope theta = 130000
</span></span><span style=display:flex><span>INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05
</span></span><span style=display:flex><span>INFO:hf-to-gguf:gguf: file type = 1
</span></span><span style=display:flex><span>INFO:hf-to-gguf:Set model tokenizer
</span></span><span style=display:flex><span>INFO:gguf.vocab:Adding 48900 merge(s).
</span></span><span style=display:flex><span>INFO:gguf.vocab:Setting special token type bos to 1
</span></span><span style=display:flex><span>INFO:gguf.vocab:Setting special token type eos to 2
</span></span><span style=display:flex><span>INFO:gguf.vocab:Setting special token type unk to 0
</span></span><span style=display:flex><span>INFO:gguf.vocab:Setting special token type pad to 2
</span></span><span style=display:flex><span>INFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if loop.first and messages[0][&#39;role&#39;] != &#39;system&#39; %}{{ &#39;&lt;|im_start|&gt;system
</span></span><span style=display:flex><span>You are a helpful AI assistant named SmolLM, trained by Hugging Face&lt;|im_end|&gt;
</span></span><span style=display:flex><span>&#39; }}{% endif %}{{&#39;&lt;|im_start|&gt;&#39; + message[&#39;role&#39;] + &#39;
</span></span><span style=display:flex><span>&#39; + message[&#39;content&#39;] + &#39;&lt;|im_end|&gt;&#39; + &#39;
</span></span><span style=display:flex><span>&#39;}}{% endfor %}{% if add_generation_prompt %}{{ &#39;&lt;|im_start|&gt;assistant
</span></span><span style=display:flex><span>&#39; }}{% endif %}
</span></span><span style=display:flex><span>INFO:hf-to-gguf:Set model quantization version
</span></span><span style=display:flex><span>INFO:gguf.gguf_writer:Writing the following files:
</span></span><span style=display:flex><span>INFO:gguf.gguf_writer:SmolLM2.gguf: n_tensors = 218, total_size = 3.4G
</span></span><span style=display:flex><span>Writing: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3.42G/3.42G [00:15&lt;00:00, 215Mbyte/s]
</span></span><span style=display:flex><span>INFO:hf-to-gguf:Model successfully exported to SmolLM2.gguf
</span></span></code></pre></div><h3 id=quantizing-the-model>quantizing the model<a href=#quantizing-the-model class=hanchor arialabel=Anchor>#</a></h3><p>Now we can finally quantize our model!
To do that, we&rsquo;ll use <code>llama-quantize</code> executable that we previously compiled with other <code>llama.cpp</code> executables.
First, let&rsquo;s check what quantizations we have available.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>llama-quantize --help
</span></span></code></pre></div><p>As of now, <code>llama-quantize --help</code> shows following types:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>
</span></span><span style=display:flex><span>usage: llama-quantize [--help] [--allow-requantize] [--leave-output-tensor] [--pure] [--imatrix] [--include-weights] [--exclude-weights] [--output-tensor-type] [--token-embedding-type] [--override-kv] model-f32.gguf [model-quant.gguf] type [nthreads]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Allowed quantization types:
</span></span><span style=display:flex><span>   2  or  Q4_0    :  4.34G, +0.4685 ppl @ Llama-3-8B
</span></span><span style=display:flex><span>   3  or  Q4_1    :  4.78G, +0.4511 ppl @ Llama-3-8B
</span></span><span style=display:flex><span>   8  or  Q5_0    :  5.21G, +0.1316 ppl @ Llama-3-8B
</span></span><span style=display:flex><span>   9  or  Q5_1    :  5.65G, +0.1062 ppl @ Llama-3-8B
</span></span><span style=display:flex><span>  19  or  IQ2_XXS :  2.06 bpw quantization
</span></span><span style=display:flex><span>  20  or  IQ2_XS  :  2.31 bpw quantization
</span></span><span style=display:flex><span>  28  or  IQ2_S   :  2.5  bpw quantization
</span></span><span style=display:flex><span>  29  or  IQ2_M   :  2.7  bpw quantization
</span></span><span style=display:flex><span>  24  or  IQ1_S   :  1.56 bpw quantization
</span></span><span style=display:flex><span>  31  or  IQ1_M   :  1.75 bpw quantization
</span></span><span style=display:flex><span>  36  or  TQ1_0   :  1.69 bpw ternarization
</span></span><span style=display:flex><span>  37  or  TQ2_0   :  2.06 bpw ternarization
</span></span><span style=display:flex><span>  10  or  Q2_K    :  2.96G, +3.5199 ppl @ Llama-3-8B
</span></span><span style=display:flex><span>  21  or  Q2_K_S  :  2.96G, +3.1836 ppl @ Llama-3-8B
</span></span><span style=display:flex><span>  23  or  IQ3_XXS :  3.06 bpw quantization
</span></span><span style=display:flex><span>  26  or  IQ3_S   :  3.44 bpw quantization
</span></span><span style=display:flex><span>  27  or  IQ3_M   :  3.66 bpw quantization mix
</span></span><span style=display:flex><span>  12  or  Q3_K    : alias for Q3_K_M
</span></span><span style=display:flex><span>  22  or  IQ3_XS  :  3.3 bpw quantization
</span></span><span style=display:flex><span>  11  or  Q3_K_S  :  3.41G, +1.6321 ppl @ Llama-3-8B
</span></span><span style=display:flex><span>  12  or  Q3_K_M  :  3.74G, +0.6569 ppl @ Llama-3-8B
</span></span><span style=display:flex><span>  13  or  Q3_K_L  :  4.03G, +0.5562 ppl @ Llama-3-8B
</span></span><span style=display:flex><span>  25  or  IQ4_NL  :  4.50 bpw non-linear quantization
</span></span><span style=display:flex><span>  30  or  IQ4_XS  :  4.25 bpw non-linear quantization
</span></span><span style=display:flex><span>  15  or  Q4_K    : alias for Q4_K_M
</span></span><span style=display:flex><span>  14  or  Q4_K_S  :  4.37G, +0.2689 ppl @ Llama-3-8B
</span></span><span style=display:flex><span>  15  or  Q4_K_M  :  4.58G, +0.1754 ppl @ Llama-3-8B
</span></span><span style=display:flex><span>  17  or  Q5_K    : alias for Q5_K_M
</span></span><span style=display:flex><span>  16  or  Q5_K_S  :  5.21G, +0.1049 ppl @ Llama-3-8B
</span></span><span style=display:flex><span>  17  or  Q5_K_M  :  5.33G, +0.0569 ppl @ Llama-3-8B
</span></span><span style=display:flex><span>  18  or  Q6_K    :  6.14G, +0.0217 ppl @ Llama-3-8B
</span></span><span style=display:flex><span>   7  or  Q8_0    :  7.96G, +0.0026 ppl @ Llama-3-8B
</span></span><span style=display:flex><span>  33  or  Q4_0_4_4 :  4.34G, +0.4685 ppl @ Llama-3-8B
</span></span><span style=display:flex><span>  34  or  Q4_0_4_8 :  4.34G, +0.4685 ppl @ Llama-3-8B
</span></span><span style=display:flex><span>  35  or  Q4_0_8_8 :  4.34G, +0.4685 ppl @ Llama-3-8B
</span></span><span style=display:flex><span>   1  or  F16     : 14.00G, +0.0020 ppl @ Mistral-7B
</span></span><span style=display:flex><span>  32  or  BF16    : 14.00G, -0.0050 ppl @ Mistral-7B
</span></span><span style=display:flex><span>   0  or  F32     : 26.00G              @ 7B
</span></span><span style=display:flex><span>          COPY    : only copy tensors, no quantizing
</span></span></code></pre></div><p>Let&rsquo;s decode this table.
From the left, we have IDs and names of quantization types - you can use either when calling <code>llama-quantize</code>.
After the <code>:</code>, there&rsquo;s a short description that in most cases shows either the example model&rsquo;s size and perplexity, or the amount of bits per tensor weight (bpw) for that specific quantization.
Perplexity is a metric that describes how certain the model is about it&rsquo;s predictions.
We can think about it like that: lower perplexity -> model is more certain about it&rsquo;s predictions -> model is more accurate.
This is a daily reminder that LLMs are nothing more than overcomplicated autocompletion algorithms.
The &ldquo;bits per weight&rdquo; metric tells us the average size of quantized tensor&rsquo;s weight.
You may think it&rsquo;s strange that those are floating-point values, but we&rsquo;ll see the reason for that soon.</p><p>Now, the main question that needs to be answered is &ldquo;which quantization do we pick?&rdquo;.
And the answer is &ldquo;it depends&rdquo;.
My rule of thumb for picking quantization type is &ldquo;the largest i can fit in my VRAM, unless it&rsquo;s too slow for my taste&rdquo; and i recommend this approach if you don&rsquo;t know where to start!
Obviously, if you don&rsquo;t have/want to use GPU, replace &ldquo;VRAM&rdquo; with &ldquo;RAM&rdquo; and &ldquo;largest i can fit&rdquo; with &ldquo;largest i can fit without forcing the OS to move everything to swap&rdquo;.
That creates another question - &ldquo;what is the largest quant i can fit in my (V)RAM&rdquo;?
And this depends on the original model&rsquo;s size and encoding, and - obviously - the amount of (V)RAM you have.
Since we&rsquo;re using SmolLM2, our model is relatively small.
GGUF file of 1.7B-Instruct variant in BF16 format weights 3.4GB.
Most models you&rsquo;ll encounter will be encoded in either BF16 or FP16 format, rarely we can find FP32-encoded LLMs.
That means most of models have 16 bits per weight by default.
We can easily approximate the size of quantized model by multiplying the original size with approximate ratio of bits per weight.
For example, let&rsquo;s assume we want to know how large will SmolLM2 1.7B-Instruct be after Q8_0 quantization.
Let&rsquo;s assume Q8_0 quant uses 8 bits per word, which means the ratio is simply 1/2, so our model should weight ~1.7GB.
Let&rsquo;s check that!</p><p>The first argument is source model, second - target file.
Third is the quantization type, and last is the amount of cores for parallel processing.
Replace <code>N</code> with amount of cores in your system and run this command:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>llama-quantize SmolLM2.gguf SmolLM2.q8.gguf Q8_0 N
</span></span></code></pre></div><p>You should see similar output:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>main: build = 4200 (46c69e0e)
</span></span><span style=display:flex><span>main: built with gcc (GCC) 14.2.1 20240910 for x86_64-pc-linux-gnu
</span></span><span style=display:flex><span>main: quantizing &#39;SmolLM2.gguf&#39; to &#39;SmolLM2.q8.gguf&#39; as Q8_0 using 24 threads
</span></span><span style=display:flex><span>llama_model_loader: loaded meta data with 37 key-value pairs and 218 tensors from SmolLM2.gguf (version GGUF V3 (latest))
</span></span><span style=display:flex><span>llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
</span></span><span style=display:flex><span>llama_model_loader: - kv   0:                       general.architecture str              = llama
</span></span><span style=display:flex><span>llama_model_loader: - kv   1:                               general.type str              = model
</span></span><span style=display:flex><span>llama_model_loader: - kv   2:                               general.name str              = SmolLM2 1.7B Instruct
</span></span><span style=display:flex><span>llama_model_loader: - kv   3:                           general.finetune str              = Instruct
</span></span><span style=display:flex><span>llama_model_loader: - kv   4:                           general.basename str              = SmolLM2
</span></span><span style=display:flex><span>llama_model_loader: - kv   5:                         general.size_label str              = 1.7B
</span></span><span style=display:flex><span>llama_model_loader: - kv   6:                            general.license str              = apache-2.0
</span></span><span style=display:flex><span>llama_model_loader: - kv   7:                   general.base_model.count u32              = 1
</span></span><span style=display:flex><span>llama_model_loader: - kv   8:                  general.base_model.0.name str              = SmolLM2 1.7B
</span></span><span style=display:flex><span>llama_model_loader: - kv   9:          general.base_model.0.organization str              = HuggingFaceTB
</span></span><span style=display:flex><span>llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/HuggingFaceTB/...
</span></span><span style=display:flex><span>llama_model_loader: - kv  11:                               general.tags arr[str,4]       = [&#34;safetensors&#34;, &#34;onnx&#34;, &#34;transformers...
</span></span><span style=display:flex><span>llama_model_loader: - kv  12:                          general.languages arr[str,1]       = [&#34;en&#34;]
</span></span><span style=display:flex><span>llama_model_loader: - kv  13:                          llama.block_count u32              = 24
</span></span><span style=display:flex><span>llama_model_loader: - kv  14:                       llama.context_length u32              = 8192
</span></span><span style=display:flex><span>llama_model_loader: - kv  15:                     llama.embedding_length u32              = 2048
</span></span><span style=display:flex><span>llama_model_loader: - kv  16:                  llama.feed_forward_length u32              = 8192
</span></span><span style=display:flex><span>llama_model_loader: - kv  17:                 llama.attention.head_count u32              = 32
</span></span><span style=display:flex><span>llama_model_loader: - kv  18:              llama.attention.head_count_kv u32              = 32
</span></span><span style=display:flex><span>llama_model_loader: - kv  19:                       llama.rope.freq_base f32              = 130000.000000
</span></span><span style=display:flex><span>llama_model_loader: - kv  20:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
</span></span><span style=display:flex><span>llama_model_loader: - kv  21:                          general.file_type u32              = 32
</span></span><span style=display:flex><span>llama_model_loader: - kv  22:                           llama.vocab_size u32              = 49152
</span></span><span style=display:flex><span>llama_model_loader: - kv  23:                 llama.rope.dimension_count u32              = 64
</span></span><span style=display:flex><span>llama_model_loader: - kv  24:                       tokenizer.ggml.model str              = gpt2
</span></span><span style=display:flex><span>llama_model_loader: - kv  25:                         tokenizer.ggml.pre str              = smollm
</span></span><span style=display:flex><span>llama_model_loader: - kv  26:                      tokenizer.ggml.tokens arr[str,49152]   = [&#34;&lt;|endoftext|&gt;&#34;, &#34;&lt;|im_start|&gt;&#34;, &#34;&lt;|...
</span></span><span style=display:flex><span>llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
</span></span><span style=display:flex><span>llama_model_loader: - kv  28:                      tokenizer.ggml.merges arr[str,48900]   = [&#34;Ġ t&#34;, &#34;Ġ a&#34;, &#34;i n&#34;, &#34;h e&#34;, &#34;Ġ Ġ...
</span></span><span style=display:flex><span>llama_model_loader: - kv  29:                tokenizer.ggml.bos_token_id u32              = 1
</span></span><span style=display:flex><span>llama_model_loader: - kv  30:                tokenizer.ggml.eos_token_id u32              = 2
</span></span><span style=display:flex><span>llama_model_loader: - kv  31:            tokenizer.ggml.unknown_token_id u32              = 0
</span></span><span style=display:flex><span>llama_model_loader: - kv  32:            tokenizer.ggml.padding_token_id u32              = 2
</span></span><span style=display:flex><span>llama_model_loader: - kv  33:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...
</span></span><span style=display:flex><span>llama_model_loader: - kv  34:            tokenizer.ggml.add_space_prefix bool             = false
</span></span><span style=display:flex><span>llama_model_loader: - kv  35:               tokenizer.ggml.add_bos_token bool             = false
</span></span><span style=display:flex><span>llama_model_loader: - kv  36:               general.quantization_version u32              = 2
</span></span><span style=display:flex><span>llama_model_loader: - type  f32:   49 tensors
</span></span><span style=display:flex><span>llama_model_loader: - type bf16:  169 tensors
</span></span><span style=display:flex><span>ggml_vulkan: Found 1 Vulkan devices:
</span></span><span style=display:flex><span>ggml_vulkan: 0 = AMD Radeon RX 7900 XT (AMD open-source driver) | uma: 0 | fp16: 1 | warp size: 64
</span></span><span style=display:flex><span>[   1/ 218]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
</span></span><span style=display:flex><span>[   2/ 218]                    token_embd.weight - [ 2048, 49152,     1,     1], type =   bf16, converting to q8_0 .. size =   192.00 MiB -&gt;   102.00 MiB
</span></span><span style=display:flex><span>[   3/ 218]                  blk.0.attn_k.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
</span></span><span style=display:flex><span>[   4/ 218]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
</span></span><span style=display:flex><span>[   5/ 218]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
</span></span><span style=display:flex><span>[   6/ 218]                  blk.0.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
</span></span><span style=display:flex><span>[   7/ 218]                  blk.0.attn_v.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>[ 212/ 218]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
</span></span><span style=display:flex><span>[ 213/ 218]                 blk.23.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
</span></span><span style=display:flex><span>[ 214/ 218]                 blk.23.attn_v.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
</span></span><span style=display:flex><span>[ 215/ 218]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
</span></span><span style=display:flex><span>[ 216/ 218]               blk.23.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
</span></span><span style=display:flex><span>[ 217/ 218]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
</span></span><span style=display:flex><span>[ 218/ 218]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
</span></span><span style=display:flex><span>llama_model_quantize_internal: model size  =  3264.38 MB
</span></span><span style=display:flex><span>llama_model_quantize_internal: quant size  =  1734.38 MB
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>main: quantize time =  2289.97 ms
</span></span><span style=display:flex><span>main:    total time =  2289.97 ms
</span></span></code></pre></div><p>And yeah, we were more-or-less correct, it&rsquo;s 1.7GB!
<strong>BUT</strong> this is only the model&rsquo;s size, we also have to consider the memory requirements for the context.
Fortunately, context doesn&rsquo;t require massive amounts of memory - i&rsquo;m not really sure how much exactly it eats up, but it&rsquo;s safe to assume that we should have at least 1GB memory for it.
So, taking all that in account, we can approximate that to load this model to memory with the context, we need at least 3GB of free (V)RAM.
This is not so bad, and most modern consumer GPUs have at least this amount (even the old GTX 1060 3GB should be able to run this model in Q8_0 quant).
However, if that&rsquo;s still too much, we can easily go lower!
Reducing the amount of bits per weight via quantization not only reduces the model&rsquo;s size, but also increases the speed of data generation.
Unfortunately, it also makes the model more stupid.
The change is gradual, you may not notice it when going from Q8_0 to Q6_K, but going below Q4 quant can be noticeable.
I strongly recommend experimenting on your own with different models and quantization types, because your experience may be different from mine!</p><blockquote><p>Oh, by the way - remember that right now our <code>llama.cpp</code> build will use CPU for calculations, so the model will reside in RAM.
Make sure you have at least 3GB of free RAM before trying to use the model, if you don&rsquo;t - quantize it with smaller quant, or get a smaller version.</p></blockquote><p>Anyway, we got our quantized model now, we can <strong>finally</strong> use it!</p><h2 id=running-llamacpp-server>running llama.cpp server<a href=#running-llamacpp-server class=hanchor arialabel=Anchor>#</a></h2><p>If going through the first part of this post felt like pain and suffering, don&rsquo;t worry - i felt the same writing it.
That&rsquo;s why it took a month to write.
But, at long last we can do something fun.</p><p>Let&rsquo;s start, as usual, with printing the help to make sure our binary is working fine:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>llama-server --help
</span></span></code></pre></div><p>You should see a lot of options.
Some of them will be explained here in a bit, some of them you&rsquo;ll have to research yourself.
For now, the only options that are interesting to us are:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>-m,    --model FNAME                    model path (default: `models/$filename` with filename from `--hf-file`
</span></span><span style=display:flex><span>                                        or `--model-url` if set, otherwise models/7B/ggml-model-f16.gguf)
</span></span><span style=display:flex><span>                                        (env: LLAMA_ARG_MODEL)
</span></span><span style=display:flex><span>--host HOST                             ip address to listen (default: 127.0.0.1)
</span></span><span style=display:flex><span>                                        (env: LLAMA_ARG_HOST)
</span></span><span style=display:flex><span>--port PORT                             port to listen (default: 8080)
</span></span><span style=display:flex><span>                                        (env: LLAMA_ARG_PORT)
</span></span></code></pre></div><p>Run <code>llama-server</code> with model&rsquo;s path set to quantized SmolLM2 GGUF file.
If you don&rsquo;t have anything running on <code>127.0.0.1:8080</code>, you can leave the host and port on defaults.
Notice that you can also use environmental variables instead of arguments, so you can setup your env and just call <code>llama-server</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>llama-server -m SmolLM2.q8.gguf
</span></span></code></pre></div><p>You should see similar output after running this command:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>build: 4182 (ab96610b) with cc (GCC) 14.2.1 20240910 for x86_64-pc-linux-gnu
</span></span><span style=display:flex><span>system info: n_threads = 12, n_threads_batch = 12, total_threads = 24
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>system_info: n_threads = 12 (n_threads_batch = 12) / 24 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 |
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>main: HTTP server is listening, hostname: 127.0.0.1, port: 8080, http threads: 23
</span></span><span style=display:flex><span>main: loading model
</span></span><span style=display:flex><span>srv    load_model: loading model &#39;SmolLM2.q8.gguf&#39;
</span></span><span style=display:flex><span>llama_model_loader: loaded meta data with 37 key-value pairs and 218 tensors from SmolLM2.q8.gguf (version GGUF V3 (latest))
</span></span><span style=display:flex><span>llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
</span></span><span style=display:flex><span>llama_model_loader: - kv   0:                       general.architecture str              = llama
</span></span><span style=display:flex><span>llama_model_loader: - kv   1:                               general.type str              = model
</span></span><span style=display:flex><span>llama_model_loader: - kv   2:                               general.name str              = SmolLM2 1.7B Instruct
</span></span><span style=display:flex><span>llama_model_loader: - kv   3:                           general.finetune str              = Instruct
</span></span><span style=display:flex><span>llama_model_loader: - kv   4:                           general.basename str              = SmolLM2
</span></span><span style=display:flex><span>llama_model_loader: - kv   5:                         general.size_label str              = 1.7B
</span></span><span style=display:flex><span>llama_model_loader: - kv   6:                            general.license str              = apache-2.0
</span></span><span style=display:flex><span>llama_model_loader: - kv   7:                   general.base_model.count u32              = 1
</span></span><span style=display:flex><span>llama_model_loader: - kv   8:                  general.base_model.0.name str              = SmolLM2 1.7B
</span></span><span style=display:flex><span>llama_model_loader: - kv   9:          general.base_model.0.organization str              = HuggingFaceTB
</span></span><span style=display:flex><span>llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/HuggingFaceTB/...
</span></span><span style=display:flex><span>llama_model_loader: - kv  11:                               general.tags arr[str,4]       = [&#34;safetensors&#34;, &#34;onnx&#34;, &#34;transformers...
</span></span><span style=display:flex><span>llama_model_loader: - kv  12:                          general.languages arr[str,1]       = [&#34;en&#34;]
</span></span><span style=display:flex><span>llama_model_loader: - kv  13:                          llama.block_count u32              = 24
</span></span><span style=display:flex><span>llama_model_loader: - kv  14:                       llama.context_length u32              = 8192
</span></span><span style=display:flex><span>llama_model_loader: - kv  15:                     llama.embedding_length u32              = 2048
</span></span><span style=display:flex><span>llama_model_loader: - kv  16:                  llama.feed_forward_length u32              = 8192
</span></span><span style=display:flex><span>llama_model_loader: - kv  17:                 llama.attention.head_count u32              = 32
</span></span><span style=display:flex><span>llama_model_loader: - kv  18:              llama.attention.head_count_kv u32              = 32
</span></span><span style=display:flex><span>llama_model_loader: - kv  19:                       llama.rope.freq_base f32              = 130000.000000
</span></span><span style=display:flex><span>llama_model_loader: - kv  20:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
</span></span><span style=display:flex><span>llama_model_loader: - kv  21:                          general.file_type u32              = 7
</span></span><span style=display:flex><span>llama_model_loader: - kv  22:                           llama.vocab_size u32              = 49152
</span></span><span style=display:flex><span>llama_model_loader: - kv  23:                 llama.rope.dimension_count u32              = 64
</span></span><span style=display:flex><span>llama_model_loader: - kv  24:                       tokenizer.ggml.model str              = gpt2
</span></span><span style=display:flex><span>llama_model_loader: - kv  25:                         tokenizer.ggml.pre str              = smollm
</span></span><span style=display:flex><span>llama_model_loader: - kv  26:                      tokenizer.ggml.tokens arr[str,49152]   = [&#34;&lt;|endoftext|&gt;&#34;, &#34;&lt;|im_start|&gt;&#34;, &#34;&lt;|...
</span></span><span style=display:flex><span>llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
</span></span><span style=display:flex><span>llama_model_loader: - kv  28:                      tokenizer.ggml.merges arr[str,48900]   = [&#34;Ġ t&#34;, &#34;Ġ a&#34;, &#34;i n&#34;, &#34;h e&#34;, &#34;Ġ Ġ...
</span></span><span style=display:flex><span>llama_model_loader: - kv  29:                tokenizer.ggml.bos_token_id u32              = 1
</span></span><span style=display:flex><span>llama_model_loader: - kv  30:                tokenizer.ggml.eos_token_id u32              = 2
</span></span><span style=display:flex><span>llama_model_loader: - kv  31:            tokenizer.ggml.unknown_token_id u32              = 0
</span></span><span style=display:flex><span>llama_model_loader: - kv  32:            tokenizer.ggml.padding_token_id u32              = 2
</span></span><span style=display:flex><span>llama_model_loader: - kv  33:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...
</span></span><span style=display:flex><span>llama_model_loader: - kv  34:            tokenizer.ggml.add_space_prefix bool             = false
</span></span><span style=display:flex><span>llama_model_loader: - kv  35:               tokenizer.ggml.add_bos_token bool             = false
</span></span><span style=display:flex><span>llama_model_loader: - kv  36:               general.quantization_version u32              = 2
</span></span><span style=display:flex><span>llama_model_loader: - type  f32:   49 tensors
</span></span><span style=display:flex><span>llama_model_loader: - type q8_0:  169 tensors
</span></span><span style=display:flex><span>llm_load_vocab: special tokens cache size = 17
</span></span><span style=display:flex><span>llm_load_vocab: token to piece cache size = 0.3170 MB
</span></span><span style=display:flex><span>llm_load_print_meta: format           = GGUF V3 (latest)
</span></span><span style=display:flex><span>llm_load_print_meta: arch             = llama
</span></span><span style=display:flex><span>llm_load_print_meta: vocab type       = BPE
</span></span><span style=display:flex><span>llm_load_print_meta: n_vocab          = 49152
</span></span><span style=display:flex><span>llm_load_print_meta: n_merges         = 48900
</span></span><span style=display:flex><span>llm_load_print_meta: vocab_only       = 0
</span></span><span style=display:flex><span>llm_load_print_meta: n_ctx_train      = 8192
</span></span><span style=display:flex><span>llm_load_print_meta: n_embd           = 2048
</span></span><span style=display:flex><span>llm_load_print_meta: n_layer          = 24
</span></span><span style=display:flex><span>llm_load_print_meta: n_head           = 32
</span></span><span style=display:flex><span>llm_load_print_meta: n_head_kv        = 32
</span></span><span style=display:flex><span>llm_load_print_meta: n_rot            = 64
</span></span><span style=display:flex><span>llm_load_print_meta: n_swa            = 0
</span></span><span style=display:flex><span>llm_load_print_meta: n_embd_head_k    = 64
</span></span><span style=display:flex><span>llm_load_print_meta: n_embd_head_v    = 64
</span></span><span style=display:flex><span>llm_load_print_meta: n_gqa            = 1
</span></span><span style=display:flex><span>llm_load_print_meta: n_embd_k_gqa     = 2048
</span></span><span style=display:flex><span>llm_load_print_meta: n_embd_v_gqa     = 2048
</span></span><span style=display:flex><span>llm_load_print_meta: f_norm_eps       = 0.0e+00
</span></span><span style=display:flex><span>llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
</span></span><span style=display:flex><span>llm_load_print_meta: f_clamp_kqv      = 0.0e+00
</span></span><span style=display:flex><span>llm_load_print_meta: f_max_alibi_bias = 0.0e+00
</span></span><span style=display:flex><span>llm_load_print_meta: f_logit_scale    = 0.0e+00
</span></span><span style=display:flex><span>llm_load_print_meta: n_ff             = 8192
</span></span><span style=display:flex><span>llm_load_print_meta: n_expert         = 0
</span></span><span style=display:flex><span>llm_load_print_meta: n_expert_used    = 0
</span></span><span style=display:flex><span>llm_load_print_meta: causal attn      = 1
</span></span><span style=display:flex><span>llm_load_print_meta: pooling type     = 0
</span></span><span style=display:flex><span>llm_load_print_meta: rope type        = 0
</span></span><span style=display:flex><span>llm_load_print_meta: rope scaling     = linear
</span></span><span style=display:flex><span>llm_load_print_meta: freq_base_train  = 130000.0
</span></span><span style=display:flex><span>llm_load_print_meta: freq_scale_train = 1
</span></span><span style=display:flex><span>llm_load_print_meta: n_ctx_orig_yarn  = 8192
</span></span><span style=display:flex><span>llm_load_print_meta: rope_finetuned   = unknown
</span></span><span style=display:flex><span>llm_load_print_meta: ssm_d_conv       = 0
</span></span><span style=display:flex><span>llm_load_print_meta: ssm_d_inner      = 0
</span></span><span style=display:flex><span>llm_load_print_meta: ssm_d_state      = 0
</span></span><span style=display:flex><span>llm_load_print_meta: ssm_dt_rank      = 0
</span></span><span style=display:flex><span>llm_load_print_meta: ssm_dt_b_c_rms   = 0
</span></span><span style=display:flex><span>llm_load_print_meta: model type       = ?B
</span></span><span style=display:flex><span>llm_load_print_meta: model ftype      = Q8_0
</span></span><span style=display:flex><span>llm_load_print_meta: model params     = 1.71 B
</span></span><span style=display:flex><span>llm_load_print_meta: model size       = 1.69 GiB (8.50 BPW)
</span></span><span style=display:flex><span>llm_load_print_meta: general.name     = SmolLM2 1.7B Instruct
</span></span><span style=display:flex><span>llm_load_print_meta: BOS token        = 1 &#39;&lt;|im_start|&gt;&#39;
</span></span><span style=display:flex><span>llm_load_print_meta: EOS token        = 2 &#39;&lt;|im_end|&gt;&#39;
</span></span><span style=display:flex><span>llm_load_print_meta: EOT token        = 0 &#39;&lt;|endoftext|&gt;&#39;
</span></span><span style=display:flex><span>llm_load_print_meta: UNK token        = 0 &#39;&lt;|endoftext|&gt;&#39;
</span></span><span style=display:flex><span>llm_load_print_meta: PAD token        = 2 &#39;&lt;|im_end|&gt;&#39;
</span></span><span style=display:flex><span>llm_load_print_meta: LF token         = 143 &#39;Ä&#39;
</span></span><span style=display:flex><span>llm_load_print_meta: EOG token        = 0 &#39;&lt;|endoftext|&gt;&#39;
</span></span><span style=display:flex><span>llm_load_print_meta: EOG token        = 2 &#39;&lt;|im_end|&gt;&#39;
</span></span><span style=display:flex><span>llm_load_print_meta: max token length = 162
</span></span><span style=display:flex><span>llm_load_tensors:   CPU_Mapped model buffer size =  1734.38 MiB
</span></span><span style=display:flex><span>................................................................................................
</span></span><span style=display:flex><span>llama_new_context_with_model: n_seq_max     = 1
</span></span><span style=display:flex><span>llama_new_context_with_model: n_ctx         = 4096
</span></span><span style=display:flex><span>llama_new_context_with_model: n_ctx_per_seq = 4096
</span></span><span style=display:flex><span>llama_new_context_with_model: n_batch       = 2048
</span></span><span style=display:flex><span>llama_new_context_with_model: n_ubatch      = 512
</span></span><span style=display:flex><span>llama_new_context_with_model: flash_attn    = 0
</span></span><span style=display:flex><span>llama_new_context_with_model: freq_base     = 130000.0
</span></span><span style=display:flex><span>llama_new_context_with_model: freq_scale    = 1
</span></span><span style=display:flex><span>llama_new_context_with_model: n_ctx_per_seq (4096) &lt; n_ctx_train (8192) -- the full capacity of the model will not be utilized
</span></span><span style=display:flex><span>llama_kv_cache_init:        CPU KV buffer size =   768.00 MiB
</span></span><span style=display:flex><span>llama_new_context_with_model: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB
</span></span><span style=display:flex><span>llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
</span></span><span style=display:flex><span>llama_new_context_with_model:        CPU compute buffer size =   280.01 MiB
</span></span><span style=display:flex><span>llama_new_context_with_model: graph nodes  = 774
</span></span><span style=display:flex><span>llama_new_context_with_model: graph splits = 1
</span></span><span style=display:flex><span>common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
</span></span><span style=display:flex><span>srv          init: initializing slots, n_slots = 1
</span></span><span style=display:flex><span>slot         init: id  0 | task -1 | new slot n_ctx_slot = 4096
</span></span><span style=display:flex><span>main: model loaded
</span></span><span style=display:flex><span>main: chat template, built_in: 1, chat_example: &#39;&lt;|im_start|&gt;system
</span></span><span style=display:flex><span>You are a helpful assistant&lt;|im_end|&gt;
</span></span><span style=display:flex><span>&lt;|im_start|&gt;user
</span></span><span style=display:flex><span>Hello&lt;|im_end|&gt;
</span></span><span style=display:flex><span>&lt;|im_start|&gt;assistant
</span></span><span style=display:flex><span>Hi there&lt;|im_end|&gt;
</span></span><span style=display:flex><span>&lt;|im_start|&gt;user
</span></span><span style=display:flex><span>How are you?&lt;|im_end|&gt;
</span></span><span style=display:flex><span>&lt;|im_start|&gt;assistant
</span></span><span style=display:flex><span>&#39;
</span></span><span style=display:flex><span>main: server is listening on http://127.0.0.1:8080 - starting the main loop
</span></span><span style=display:flex><span>srv  update_slots: all slots are idle
</span></span></code></pre></div><p>And now we can access the web UI on <code>http://127.0.0.1:8080</code> or whatever host/port combo you&rsquo;ve set.</p><p><img alt="llama.cpp webui" src=/img/llama-cpp/llama-cpp-webui.png></p><p>From this point, we can freely chat with the LLM using the web UI, or you can use the OpenAI-compatible API that <code>llama-server</code> provides.
I won&rsquo;t dig into the API itself here, i&rsquo;ve written <a href=https://github.com/SteelPh0enix/unreasonable-llama>a Python library</a> for it if you&rsquo;re interested in using it (i&rsquo;m trying to keep it up-to-date with <code>llama.cpp</code> master, but it might not be all the time).
I recommend looking into the <a href=https://github.com/ggerganov/llama.cpp/tree/master/examples/server><code>llama-server</code> source code and README</a> for more details about endpoints.</p><p>Let&rsquo;s see what we can do with web UI.
On the left, we have list of conversations.
Those are stored in browser&rsquo;s localStorage (as the disclaimer on the bottom-left graciously explains), which means they are persistent even if you restart the browser.
Keep in mind that changing the host/port of the server will &ldquo;clear&rdquo; those.
Current conversation is passed to the LLM as context, and the context size is limited by server settings (we will learn how to tweak it in a second).
I recommend making new conversations often, and keeping their context focused on the subject for optimal performance.</p><p>On the top-right, we have (from left to right) &ldquo;remove conversation&rdquo;, &ldquo;download conversation&rdquo; (in JSON format), &ldquo;configuration&rdquo; and &ldquo;Theme&rdquo; buttons.
In the configuration window, we can tweak generation settings for our LLM.
<strong>Those are currently global, not per-conversation.</strong>
All of those settings are briefly described <a href=/posts/llama-cpp-guide/#llm-configuration-options-explained>below</a>.</p><p><img alt="llama.cpp webui config" src=/img/llama-cpp/llama-cpp-webui-config.png></p><h3 id=llamacpp-server-settings>llama.cpp server settings<a href=#llamacpp-server-settings class=hanchor arialabel=Anchor>#</a></h3><p>Web UI provides only a small subset of configuration options we have available, and only those related to LLM samplers.
For the full set, we need to call <code>llama-server --help</code>.
With those options we can drastically improve (or worsen) the behavior of our model and performance of text generation, so it&rsquo;s worth knowing them.
I won&rsquo;t explain <em>all</em> of the options listed there, because it would be mostly redundant, but i&rsquo;ll probably explain <em>most</em> of them in one way of another here.
I&rsquo;ll try to explain all <em>interesting</em> options though.</p><p>One thing that&rsquo;s also worth mentioning is that most of those parameters are read from the environment.
This is also the case for most other <code>llama.cpp</code> executables, and the parameter names (and environment variables) are the same for them.
Names of those variables are provided in <code>llama-server --help</code> output, i&rsquo;ll add them to each described option here.</p><p>Let&rsquo;s start with <code>common params</code> section:</p><ul><li><code>--threads</code>/<code>--threads-batch</code> (<code>LLAMA_ARG_THREADS</code>) - amount of CPU threads used by LLM.
Default value is -1, which tells <code>llama.cpp</code> to detect the amount of cores in the system.
This behavior is probably good enough for most of people, so unless you have <em>exotic</em> hardware setup and you know what you&rsquo;re doing - leave it on default.
If you <em>do</em> have an exotic setup, you may also want to look at other NUMA and offloading-related flags.</li><li><code>--ctx-size</code> (<code>LLAMA_ARG_CTX_SIZE</code>) - size of the prompt context.
In other words, the amount of tokens that the LLM can remember at once.
Increasing the context size also increases the memory requirements for the LLM.
Every model has a context size limit, when this argument is set to <code>0</code>, <code>llama.cpp</code> tries to use it.</li><li><code>--predict</code> (<code>LLAMA_ARG_N_PREDICT</code>) - number of tokens to predict.
When LLM generates text, it stops either after generating end-of-message token (when it decides that the generated sentence is over), or after hitting this limit.
Default is <code>-1</code>, which makes the LLM generate text ad infinitum.
If we want to limit it to context size, we can set it to <code>-2</code>.</li><li><code>--batch-size</code>/<code>--ubatch-size</code> (<code>LLAMA_ARG_BATCH</code>/<code>LLAMA_ARG_UBATCH</code>) - amount of tokens fed to the LLM in single processing step.
Optimal value of those arguments depends on your hardware, model, and context size - i encourage experimentation, but defaults are probably good enough for start.</li><li><code>--flash-attn</code> (<code>LLAMA_ARG_FLASH_ATTN</code>) - <a href=https://www.hopsworks.ai/dictionary/flash-attention>Flash attention</a> is an optimization that&rsquo;s supported by most recent models.
Read the linked article for details, in short - enabling it should improve the generation performance for some models.
<code>llama.cpp</code> will simply throw a warning when a model that doesn&rsquo;t support flash attention is loaded, so i keep it on at all times without any issues.</li><li><code>--mlock</code> (<code>LLAMA_ARG_MLOCK</code>) - this option is called exactly like <a href=https://man7.org/linux/man-pages/man2/mlock.2.html>Linux function</a> that it uses underneath.
On Windows, it uses <a href=https://learn.microsoft.com/en-us/windows/win32/api/memoryapi/nf-memoryapi-virtuallock>VirtualLock</a>.
If you have enough virtual memory (RAM or VRAM) to load the whole model into, you can use this parameter to prevent OS from swapping it to the hard drive.
Enabling it can increase the performance of text generation, but may slow everything else down in return if you hit the virtual memory limit of your machine.</li><li><code>--no-mmap</code> (<code>LLAMA_ARG_NO_MMAP</code>) - by default, <code>llama.cpp</code> will map the model to memory (using <a href=https://man7.org/linux/man-pages/man2/mmap.2.html><code>mmap</code></a> on Linux and <a href=https://learn.microsoft.com/en-us/windows/win32/api/winbase/nf-winbase-createfilemappinga><code>CreateFileMappingA</code></a> on Windows).
Using this switch will disable this behavior.</li><li><code>--gpu-layers</code> (<code>LLAMA_ARG_N_GPU_LAYERS</code>) - if GPU offloading is available, this parameter will set the maximum amount of LLM layers to offload to GPU.
Number and size of layers is dependent on the used model.
Usually, if we want to load the whole model to GPU, we can set this parameter to some unreasonably large number like 999.
For partial offloading, you must experiment yourself.
<code>llama.cpp</code> must be built with GPU support, otherwise this option will have no effect.
If you have multiple GPUs, you may also want to look at <code>--split-mode</code> and <code>--main-gpu</code> arguments.</li><li><code>--model</code> (<code>LLAMA_ARG_MODEL</code>) - path to the GGUF model file.</li></ul><p>Most of the options from <code>sampling params</code> section are described in detail <a href=/posts/llama-cpp-guide/#list-of-llm-configuration-options-and-samplers-available-in-llamacpp>below</a>.
Server-specific arguments are:</p><ul><li><code>--no-context-shift</code> (<code>LLAMA_ARG_NO_CONTEXT_SHIFT</code>) - by default, when context is filled up, it will be shifted (&ldquo;oldest&rdquo; tokens are discarded in favour of freshly generated ones).
This parameter disables that behavior, and it will make the generation stop instead.</li><li><code>--cont-batching</code> (<code>LLAMA_ARG_CONT_BATCHING</code>) - continuous batching allows processing prompts in parallel with text generation.
This usually improves performance and is enabled by default.
You can disable it with <code>--no-cont-batching</code> (<code>LLAMA_ARG_NO_CONT_BATCHING</code>) parameter.</li><li><code>--alias</code> (<code>LLAMA_ARG_ALIAS</code>) - Alias for the model name, used by the REST API.
Set to model name by default.</li><li><code>--host</code> (<code>LLAMA_ARG_HOST</code>) and <code>--port</code> (<code>LLAMA_ARG_PORT</code>) - host and port for <code>llama.cpp</code> server.</li><li><code>--slots</code> (<code>LLAMA_ARG_ENDPOINT_SLOTS</code>) - enables <code>/slots</code> endpoint of <code>llama.cpp</code> server.</li><li><code>--props</code> (<code>LLAMA_ARG_ENDPOINT_PROPS</code>) - enables <code>/props</code> endpoint of <code>llama.cpp</code> server.</li></ul><h2 id=other-llamacpp-tools>other llama.cpp tools<a href=#other-llamacpp-tools class=hanchor arialabel=Anchor>#</a></h2><p>Webserver is not the only thing <code>llama.cpp</code> provides.
There&rsquo;s few other useful tools hidden in built binaries.</p><h3 id=llama-bench><code>llama-bench</code><a href=#llama-bench class=hanchor arialabel=Anchor>#</a></h3><p><code>llama-bench</code> allows us to benchmark the prompt processing and text generation speed of our <code>llama.cpp</code> build for a selected model.
To run an example benchmark, we can simply run the executable with path to selected model.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>llama-bench --model selected_model.gguf
</span></span></code></pre></div><p><code>llama-bench</code> will try to use optimal <code>llama.cpp</code> configuration for your hardware.
Default settings will try to use full GPU offloading (99 layers) and mmap.
I recommend enabling flash attention manually (with <code>--flash-attn</code> flag, unfortunately <code>llama-bench</code> does not read the environmental variables)
Tweaking prompt length (<code>--n-prompt</code>) and batch sizes (<code>--batch-size</code>/<code>--ubatch-size</code>) may affect the result of prompt processing benchmark.
Tweaking number of tokens to generate (<code>--n-gen</code>) may affect the result of text generation benchmark.
You can also set the number of repetitions with <code>--repetitions</code> argument.</p><p>Results for SmolLM2 1.7B Instruct quantized to Q8 w/ flash attention on my setup (CPU only, Ryzen 5900X, DDR4 RAM @3200MHz):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>&gt; llama-bench --flash-attn 1 --model ./SmolLM2.q8.gguf
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>| model                          |       size |     params | backend    | threads | fa |          test |                  t/s |
</span></span><span style=display:flex><span>| ------------------------------ | ---------: | ---------: | ---------- | ------: | -: | ------------: | -------------------: |
</span></span><span style=display:flex><span>| llama ?B Q8_0                  |   1.69 GiB |     1.71 B | CPU        |      12 |  1 |         pp512 |        162.54 ± 1.70 |
</span></span><span style=display:flex><span>| llama ?B Q8_0                  |   1.69 GiB |     1.71 B | CPU        |      12 |  1 |         tg128 |         22.50 ± 0.05 |
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>build: dc223440 (4215)
</span></span></code></pre></div><p>The table is mostly self-explanatory, except two last columns.
<code>test</code> contains the benchmark identifier, made from two parts.
First two letters define the bench type (<code>pp</code> for prompt processing, <code>tg</code> for text generation).
The number defines the prompt size (for prompt processing benchmark) or amount of generated tokens (for text generation benchmark).
<code>t/s</code> column is the result in tokens processed/generated per second.</p><p>There&rsquo;s also a <em>mystery</em> <code>-pg</code> argument, which can be used to perform mixed prompt processing+text generation test.
For example:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>&gt; llama-bench --flash-attn 1 --model ./SmolLM2.q8.gguf -pg 1024,256
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>| model                          |       size |     params | backend    | threads | fa |          test |                  t/s |
</span></span><span style=display:flex><span>| ------------------------------ | ---------: | ---------: | ---------- | ------: | -: | ------------: | -------------------: |
</span></span><span style=display:flex><span>| llama ?B Q8_0                  |   1.69 GiB |     1.71 B | CPU        |      12 |  1 |         pp512 |        165.50 ± 1.95 |
</span></span><span style=display:flex><span>| llama ?B Q8_0                  |   1.69 GiB |     1.71 B | CPU        |      12 |  1 |         tg128 |         22.44 ± 0.01 |
</span></span><span style=display:flex><span>| llama ?B Q8_0                  |   1.69 GiB |     1.71 B | CPU        |      12 |  1 |  pp1024+tg256 |         63.51 ± 4.24 |
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>build: dc223440 (4215)
</span></span></code></pre></div><p>This is probably the most realistic benchmark, because as long as you have continuous batching enabled you&rsquo;ll use the model like that.</p><h3 id=llama-cli><code>llama-cli</code><a href=#llama-cli class=hanchor arialabel=Anchor>#</a></h3><p>This is a simple CLI interface for the LLM.
It allows you to generate a completion for specified prompt, or chat with the LLM.</p><p>It shares most arguments with <code>llama-server</code>, except some specific ones:</p><ul><li><code>--prompt</code> - can also be used with <code>llama-server</code>, but here it&rsquo;s bit more useful.
Sets the starting/system prompt for the LLM.
Prompt can also be loaded from file by specifying it&rsquo;s path using <code>--file</code> or <code>--binary-file</code> argument.</li><li><code>--color</code> - enables colored output, it&rsquo;s disabled by default.</li><li><code>--no-context-shift</code> (<code>LLAMA_ARG_NO_CONTEXT_SHIFT</code>) - does the same thing as in <code>llama-server</code>.</li><li><code>--reverse-prompt</code> - when LLM generates a reverse prompt, it stops generation and returns the control over conversation to the user, allowing him to respond.
Basically, this is list of stopping words/sentences.</li><li><code>--conversation</code> - enables conversation mode by enabling interactive mode and not printing special tokens (like those appearing in chat template)
This is probably how you want to use this program.</li><li><code>--interactive</code> - enables interactive mode, allowing you to chat with the LLM. In this mode, the generation starts right away and you should set the <code>--prompt</code> to get any reasonable output.
Alternatively, we can use <code>--interactive-first</code> to start chatting with control over chat right away.</li></ul><p>Here are specific usage examples:</p><h4 id=text-completion>text completion<a href=#text-completion class=hanchor arialabel=Anchor>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>&gt; llama-cli --flash-attn --model ./SmolLM2.q8.gguf --prompt &#34;The highest mountain on earth&#34;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>build: 4215 (dc223440) with cc (GCC) 14.2.1 20240910 for x86_64-pc-linux-gnu
</span></span><span style=display:flex><span>main: llama backend init
</span></span><span style=display:flex><span>main: load the model and apply lora adapter, if any
</span></span><span style=display:flex><span>llama_model_loader: loaded meta data with 37 key-value pairs and 218 tensors from ./SmolLM2.q8.gguf (version GGUF V3 (latest))
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>llm_load_tensors:   CPU_Mapped model buffer size =  1734,38 MiB
</span></span><span style=display:flex><span>................................................................................................
</span></span><span style=display:flex><span>llama_new_context_with_model: n_seq_max     = 1
</span></span><span style=display:flex><span>llama_new_context_with_model: n_ctx         = 4096
</span></span><span style=display:flex><span>llama_new_context_with_model: n_ctx_per_seq = 4096
</span></span><span style=display:flex><span>llama_new_context_with_model: n_batch       = 2048
</span></span><span style=display:flex><span>llama_new_context_with_model: n_ubatch      = 512
</span></span><span style=display:flex><span>llama_new_context_with_model: flash_attn    = 1
</span></span><span style=display:flex><span>llama_new_context_with_model: freq_base     = 130000,0
</span></span><span style=display:flex><span>llama_new_context_with_model: freq_scale    = 1
</span></span><span style=display:flex><span>llama_new_context_with_model: n_ctx_per_seq (4096) &lt; n_ctx_train (8192) -- the full capacity of the model will not be utilized
</span></span><span style=display:flex><span>llama_kv_cache_init:        CPU KV buffer size =   768,00 MiB
</span></span><span style=display:flex><span>llama_new_context_with_model: KV self size  =  768,00 MiB, K (f16):  384,00 MiB, V (f16):  384,00 MiB
</span></span><span style=display:flex><span>llama_new_context_with_model:        CPU  output buffer size =     0,19 MiB
</span></span><span style=display:flex><span>llama_new_context_with_model:        CPU compute buffer size =   104,00 MiB
</span></span><span style=display:flex><span>llama_new_context_with_model: graph nodes  = 679
</span></span><span style=display:flex><span>llama_new_context_with_model: graph splits = 1
</span></span><span style=display:flex><span>common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
</span></span><span style=display:flex><span>main: llama threadpool init, n_threads = 12
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>system_info: n_threads = 12 (n_threads_batch = 12) / 24 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 |
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>sampler seed: 2734556630
</span></span><span style=display:flex><span>sampler params:
</span></span><span style=display:flex><span>        repeat_last_n = 64, repeat_penalty = 1,000, frequency_penalty = 0,000, presence_penalty = 0,000
</span></span><span style=display:flex><span>        dry_multiplier = 0,000, dry_base = 1,750, dry_allowed_length = 2, dry_penalty_last_n = -1
</span></span><span style=display:flex><span>        top_k = 40, top_p = 0,950, min_p = 0,050, xtc_probability = 0,000, xtc_threshold = 0,100, typical_p = 1,000, temp = 0,800
</span></span><span style=display:flex><span>        mirostat = 0, mirostat_lr = 0,100, mirostat_ent = 5,000
</span></span><span style=display:flex><span>sampler chain: logits -&gt; logit-bias -&gt; penalties -&gt; dry -&gt; top-k -&gt; typical -&gt; top-p -&gt; min-p -&gt; xtc -&gt; temp-ext -&gt; dist
</span></span><span style=display:flex><span>generate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 0
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>The highest mountain on earth is Mount Everest, which stands at an astonishing 8,848.86 meters (29,031.7 feet) above sea level. Located in the Mahalangur Sharhungtrigangla Range in the Himalayas, it&#39;s a marvel of nature that draws adventurers and thrill-seekers from around the globe.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Standing at the base camp, the mountain appears as a majestic giant, its rugged slopes and snow-capped peaks a testament to its formidable presence. The climb to the summit is a grueling challenge that requires immense physical and mental fortitude, as climbers must navigate steep inclines, unpredictable weather, and crevasses.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>The ascent begins at Base Camp, a bustling hub of activity, where climbers gather to share stories, exchange tips, and prepare for the climb ahead. From Base Camp, climbers make their way to the South Col, a precarious route that offers breathtaking views of the surrounding landscape. The final push to the summit involves a grueling ascent up the steep and treacherous Lhotse Face, followed by a scramble up the near-vertical wall of the Western Cwm.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Upon reaching the summit, climbers are rewarded with an unforgettable sight: the majestic Himalayan range unfolding before them, with the sun casting a golden glow on the snow. The sense of accomplishment and awe is indescribable, and the experience is etched in the memories of those who have conquered this mighty mountain.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>The climb to Everest is not just about reaching the summit; it&#39;s an adventure that requires patience, perseverance, and a deep respect for the mountain. Climbers must be prepared to face extreme weather conditions, altitude sickness, and the ever-present risk of accidents or crevasses. Despite these challenges, the allure of Everest remains a powerful draw, inspiring countless individuals to push their limits and push beyond them. [end of text]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>llama_perf_sampler_print:    sampling time =      12,58 ms /   385 runs   (    0,03 ms per token, 30604,13 tokens per second)
</span></span><span style=display:flex><span>llama_perf_context_print:        load time =     318,81 ms
</span></span><span style=display:flex><span>llama_perf_context_print: prompt eval time =      59,26 ms /     5 tokens (   11,85 ms per token,    84,38 tokens per second)
</span></span><span style=display:flex><span>llama_perf_context_print:        eval time =   17797,98 ms /   379 runs   (   46,96 ms per token,    21,29 tokens per second)
</span></span><span style=display:flex><span>llama_perf_context_print:       total time =   17891,23 ms /   384 tokens
</span></span></code></pre></div><h4 id=chat-mode>chat mode<a href=#chat-mode class=hanchor arialabel=Anchor>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>&gt; llama-cli --flash-attn --model ./SmolLM2.q8.gguf --prompt &#34;You are a helpful assistant&#34; --conversation
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>build: 4215 (dc223440) with cc (GCC) 14.2.1 20240910 for x86_64-pc-linux-gnu
</span></span><span style=display:flex><span>main: llama backend init
</span></span><span style=display:flex><span>main: load the model and apply lora adapter, if any
</span></span><span style=display:flex><span>llama_model_loader: loaded meta data with 37 key-value pairs and 218 tensors from ./SmolLM2.q8.gguf (version GGUF V3 (latest))
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>llm_load_tensors:   CPU_Mapped model buffer size =  1734,38 MiB
</span></span><span style=display:flex><span>................................................................................................
</span></span><span style=display:flex><span>llama_new_context_with_model: n_seq_max     = 1
</span></span><span style=display:flex><span>llama_new_context_with_model: n_ctx         = 4096
</span></span><span style=display:flex><span>llama_new_context_with_model: n_ctx_per_seq = 4096
</span></span><span style=display:flex><span>llama_new_context_with_model: n_batch       = 2048
</span></span><span style=display:flex><span>llama_new_context_with_model: n_ubatch      = 512
</span></span><span style=display:flex><span>llama_new_context_with_model: flash_attn    = 1
</span></span><span style=display:flex><span>llama_new_context_with_model: freq_base     = 130000,0
</span></span><span style=display:flex><span>llama_new_context_with_model: freq_scale    = 1
</span></span><span style=display:flex><span>llama_new_context_with_model: n_ctx_per_seq (4096) &lt; n_ctx_train (8192) -- the full capacity of the model will not be utilized
</span></span><span style=display:flex><span>llama_kv_cache_init:        CPU KV buffer size =   768,00 MiB
</span></span><span style=display:flex><span>llama_new_context_with_model: KV self size  =  768,00 MiB, K (f16):  384,00 MiB, V (f16):  384,00 MiB
</span></span><span style=display:flex><span>llama_new_context_with_model:        CPU  output buffer size =     0,19 MiB
</span></span><span style=display:flex><span>llama_new_context_with_model:        CPU compute buffer size =   104,00 MiB
</span></span><span style=display:flex><span>llama_new_context_with_model: graph nodes  = 679
</span></span><span style=display:flex><span>llama_new_context_with_model: graph splits = 1
</span></span><span style=display:flex><span>common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
</span></span><span style=display:flex><span>main: llama threadpool init, n_threads = 12
</span></span><span style=display:flex><span>main: chat template example:
</span></span><span style=display:flex><span>&lt;|im_start|&gt;system
</span></span><span style=display:flex><span>You are a helpful assistant&lt;|im_end|&gt;
</span></span><span style=display:flex><span>&lt;|im_start|&gt;user
</span></span><span style=display:flex><span>Hello&lt;|im_end|&gt;
</span></span><span style=display:flex><span>&lt;|im_start|&gt;assistant
</span></span><span style=display:flex><span>Hi there&lt;|im_end|&gt;
</span></span><span style=display:flex><span>&lt;|im_start|&gt;user
</span></span><span style=display:flex><span>How are you?&lt;|im_end|&gt;
</span></span><span style=display:flex><span>&lt;|im_start|&gt;assistant
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>system_info: n_threads = 12 (n_threads_batch = 12) / 24 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 |
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>main: interactive mode on.
</span></span><span style=display:flex><span>sampler seed: 968968654
</span></span><span style=display:flex><span>sampler params:
</span></span><span style=display:flex><span>        repeat_last_n = 64, repeat_penalty = 1,000, frequency_penalty = 0,000, presence_penalty = 0,000
</span></span><span style=display:flex><span>        dry_multiplier = 0,000, dry_base = 1,750, dry_allowed_length = 2, dry_penalty_last_n = -1
</span></span><span style=display:flex><span>        top_k = 40, top_p = 0,950, min_p = 0,050, xtc_probability = 0,000, xtc_threshold = 0,100, typical_p = 1,000, temp = 0,800
</span></span><span style=display:flex><span>        mirostat = 0, mirostat_lr = 0,100, mirostat_ent = 5,000
</span></span><span style=display:flex><span>sampler chain: logits -&gt; logit-bias -&gt; penalties -&gt; dry -&gt; top-k -&gt; typical -&gt; top-p -&gt; min-p -&gt; xtc -&gt; temp-ext -&gt; dist
</span></span><span style=display:flex><span>generate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 0
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>== Running in interactive mode. ==
</span></span><span style=display:flex><span> - Press Ctrl+C to interject at any time.
</span></span><span style=display:flex><span> - Press Return to return control to the AI.
</span></span><span style=display:flex><span> - To return control without starting a new line, end your input with &#39;/&#39;.
</span></span><span style=display:flex><span> - If you want to submit another line, end your input with &#39;\&#39;.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>system
</span></span><span style=display:flex><span>You are a helpful assistant
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>&gt; hi
</span></span><span style=display:flex><span>Hello! How can I help you today?
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>&gt;
</span></span><span style=display:flex><span>llama_perf_sampler_print:    sampling time =       0,27 ms /    22 runs   (    0,01 ms per token, 80291,97 tokens per second)
</span></span><span style=display:flex><span>llama_perf_context_print:        load time =     317,46 ms
</span></span><span style=display:flex><span>llama_perf_context_print: prompt eval time =    2043,02 ms /    22 tokens (   92,86 ms per token,    10,77 tokens per second)
</span></span><span style=display:flex><span>llama_perf_context_print:        eval time =     407,66 ms /     9 runs   (   45,30 ms per token,    22,08 tokens per second)
</span></span><span style=display:flex><span>llama_perf_context_print:       total time =    5302,60 ms /    31 tokens
</span></span><span style=display:flex><span>Interrupted by user
</span></span></code></pre></div><h2 id=building-the-llama-but-better>building the llama, but better<a href=#building-the-llama-but-better class=hanchor arialabel=Anchor>#</a></h2><p>All right, now that we know how to use <code>llama.cpp</code> and tweak runtime parameters, let&rsquo;s learn how to tweak build configuration.
We already set some generic settings in <a href=/posts/llama-cpp-guide/#building-the-llama>chapter about building the <code>llama.cpp</code></a> but we haven&rsquo;t touched any backend-related ones yet.</p><p>Let&rsquo;s start with clearing up the <code>llama.cpp</code> repository (and, optionally, making sure that we have the latest commit):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>cd llama.cpp
</span></span><span style=display:flex><span>git clean -xdf
</span></span><span style=display:flex><span>git pull
</span></span><span style=display:flex><span>git submodule update --recursive
</span></span></code></pre></div><p>Now, we need to generate the build files for a custom backend.
As of writing this, the list of backends supported by <code>llama.cpp</code> is following:</p><ul><li><code>Metal</code> - acceleration for Apple Silicon</li><li><code>Accelerate</code> - BLAS (Basic Linear Algebra Subprograms) acceleration for Mac PCs, enabled by default.</li><li><code>OpenBLAS</code> - BLAS acceleration for CPUs</li><li><code>BLIS</code> - relatively recently released high-performance BLAS framework</li><li><code>SYCL</code> - acceleration for Intel GPUs (Data Center Max series, Flex series, Arc series, Built-in GPUs and iGPUs)</li><li><code>Intel oneMKL</code> - acceleration for Intel CPUs</li><li><code>CUDA</code> - acceleration for Nvidia GPUs</li><li><code>MUSA</code> - acceleration for Moore Threads GPUs</li><li><code>hipBLAS</code> - BLAS acceleration for AMD GPUs</li><li><code>Vulkan</code> - generic acceleration for GPUs</li><li><code>CANN</code> - acceleration for Ascend NPU</li><li><code>Android</code> - yes, there&rsquo;s also Android support.</li></ul><p>As we can see, there&rsquo;s something for everyone.
My backend selection recommendation is following:</p><ul><li>Users without GPUs should try <code>Intel oneMKL</code> in case of Intel CPUs, or <code>BLIS</code>/<code>OpenBLAS</code>.</li><li>Users with Nvidia GPUs should use <code>CUDA</code> or <code>Vulkan</code></li><li>Users with AMD GPUs should use <code>Vulkan</code> or <code>ROCm</code> (order important here, ROCm was bugged last time i&rsquo;ve used it)</li><li>Users with Intel GPUs should use <code>SYCL</code> or <code>Vulkan</code></li></ul><p>As we can see, Vulkan is the most generic option for GPU acceleration and i believe it&rsquo;s the simplest to build for, so i&rsquo;ll explain in detail how to do that.
The build process for every backend is very similar - install the necessary dependencies, generate the <code>llama.cpp</code> build files with proper flag to enable the specific backend, and build it.</p><p>Oh, and don&rsquo;t worry about Python and it&rsquo;s dependencies.
The performance of model conversion scripts is not limited by <code>pytorch</code>, so there&rsquo;s no point in installing CUDA/ROCm versions.</p><p>Before generating the build file, we need to install <a href=https://www.lunarg.com/vulkan-sdk/>Vulkan SDK</a>.</p><p class=windows-bg-padded>On Windows, it&rsquo;s easiest to do via MSYS.
That&rsquo;s why i&rsquo;ve recommended using it at the beginning.
I have tried installing it directly on Windows, but encountered issues that i haven&rsquo;t seen when using MSYS - so, obviously, MSYS is a better option.
Run this command in MSYS (make sure to use UCRT runtime) to install the required dependencies:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>pacman -S git <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    mingw-w64-ucrt-x86_64-gcc <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    mingw-w64-ucrt-x86_64-cmake <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    mingw-w64-ucrt-x86_64-vulkan-devel <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    mingw-w64-ucrt-x86_64-shaderc
</span></span></code></pre></div><p class=windows-bg-padded>If you <em>really</em> don&rsquo;t want to use MSYS, i recommend <a href=https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md#vulkan>following the docs</a></p><p class=linux-bg-padded>On Linux, i recommend installing Vulkan SDK using the package manager.
If it&rsquo;s not in package manager of your distro, i assume you know what you&rsquo;re doing and how to install it manually.</p><p>Afterwards, we can generate the build files (replace <code>/your/install/dir</code> with custom installation directory, if you want):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>cmake -S . -B build -G Ninja -DGGML_VULKAN<span style=color:#f92672>=</span>ON -DCMAKE_BUILD_TYPE<span style=color:#f92672>=</span>Release -DCMAKE_INSTALL_PREFIX<span style=color:#f92672>=</span>/your/install/dir -DLLAMA_BUILD_TESTS<span style=color:#f92672>=</span>OFF -DLLAMA_BUILD_EXAMPLES<span style=color:#f92672>=</span>ON -DLLAMA_BUILD_SERVER<span style=color:#f92672>=</span>ON
</span></span></code></pre></div><p>and build/install the binaries (replace <code>X</code> with amount of cores in your system):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>cmake --build build --config Release -j X
</span></span><span style=display:flex><span>cmake --install build --config Release
</span></span></code></pre></div><p>Don&rsquo;t mind the warnings you&rsquo;ll see, i get them too.
Now our <code>llama.cpp</code> binaries should be able to use our GPU.
We can test it by running <code>llama-server</code> or <code>llama-cli</code> with <code>--list-devices</code> argument:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>&gt; llama-cli --list-devices
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ggml_vulkan: Found 1 Vulkan devices:
</span></span><span style=display:flex><span>ggml_vulkan: 0 = AMD Radeon RX 7900 XT (AMD open-source driver) | uma: 0 | fp16: 1 | warp size: 64
</span></span><span style=display:flex><span>Available devices:
</span></span><span style=display:flex><span>  Vulkan0: AMD Radeon RX 7900 XT (20464 MiB, 20464 MiB free)
</span></span></code></pre></div><p>Running that command previously would print an empty list:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>&gt; llama-cli --list-devices
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Available devices:
</span></span></code></pre></div><p>Remember the <code>llama-bench</code> results i&rsquo;ve got previously on CPU build?
This is them now:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>&gt; llama-bench --flash-attn 1 --model ./SmolLM2.q8.gguf -pg 1024,256
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ggml_vulkan: Found 1 Vulkan devices:
</span></span><span style=display:flex><span>ggml_vulkan: 0 = AMD Radeon RX 7900 XT (AMD open-source driver) | uma: 0 | fp16: 1 | warp size: 64
</span></span><span style=display:flex><span>| model                          |       size |     params | backend    | ngl | fa |          test |                  t/s |
</span></span><span style=display:flex><span>| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | ------------: | -------------------: |
</span></span><span style=display:flex><span>ggml_vulkan: Compiling shaders..............................Done!
</span></span><span style=display:flex><span>| llama ?B Q8_0                  |   1.69 GiB |     1.71 B | Vulkan     |  99 |  1 |         pp512 |        880.55 ± 5.30 |
</span></span><span style=display:flex><span>| llama ?B Q8_0                  |   1.69 GiB |     1.71 B | Vulkan     |  99 |  1 |         tg128 |         89.78 ± 1.66 |
</span></span><span style=display:flex><span>| llama ?B Q8_0                  |   1.69 GiB |     1.71 B | Vulkan     |  99 |  1 |  pp1024+tg256 |        115.25 ± 0.83 |
</span></span></code></pre></div><p>Now, that&rsquo;s some <em>good shit</em> right here.
Prompt processing speed has increased from ~165 to ~880 tokens per second (5.3x faster).
Text generation - from ~22 to ~90 tokens per second (4x faster).
Mixed text processing went up from ~63 to ~115 tokens per second (1.8x faster).
All of this due to the fact that i&rsquo;ve switched to <em>correct</em> backend.</p><h2 id=llm-configuration-options-explained>LLM configuration options explained<a href=#llm-configuration-options-explained class=hanchor arialabel=Anchor>#</a></h2><p>This will be a relatively long and very informational part full of boring explanations.
But - it&rsquo;s a good knowledge to have when playing with LLMs.</p><h3 id=how-does-llm-generate-text>how does LLM generate text?<a href=#how-does-llm-generate-text class=hanchor arialabel=Anchor>#</a></h3><ol><li><p>Prompt</p><p>Everything starts with a prompt.
Prompt can be a simple raw string that we want the LLM to complete for us, or it can be an elaborate construction that allows the LLM to chat or use external tools.
Whatever we put in it, it&rsquo;s usually in human-readable format with special &ldquo;tags&rdquo; (usually similar to XML tags) used for separating the parts of the prompt.</p><p>We&rsquo;ve already seen an example of a prompt used for chat completion, provided by <code>llama-server</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>&lt;|im_start|&gt;system
</span></span><span style=display:flex><span>You are a helpful assistant&lt;|im_end|&gt;
</span></span><span style=display:flex><span>&lt;|im_start|&gt;user
</span></span><span style=display:flex><span>Hello&lt;|im_end|&gt;
</span></span><span style=display:flex><span>&lt;|im_start|&gt;assistant
</span></span><span style=display:flex><span>Hi there&lt;|im_end|&gt;
</span></span><span style=display:flex><span>&lt;|im_start|&gt;user
</span></span><span style=display:flex><span>How are you?&lt;|im_end|&gt;
</span></span><span style=display:flex><span>&lt;|im_start|&gt;assistant
</span></span></code></pre></div><p>(if you&rsquo;re wondering <em>what are those funny &lt;| and |> symbols</em> - those are ligatures from Fira Code font made out of <code>|</code>, <code>></code> and <code>&lt;</code> characters)</p></li><li><p>Tokenization</p><p>The LLM does not understand the human language like we do.
We use words and punctuation marks to form sentences - LLMs use tokens that can be understood as an equivalent to those.
First step in text generation is breaking the language barrier by performing prompt tokenization.
Tokenization is a process of translating input text (in human-readable format) into an array of tokens that can be processed by an LLM.
Tokens are simple numeric values, and with a vocabulary they can be easily mapped to their string representations (at least in case of BPE models, don&rsquo;t know about others).
In fact, that vocabulary is available in SmolLM2 repository, in <code>tokenizer.json</code> file!
That file also contains some metadata for <em>special</em> tokens that have <em>special</em> meaning for the LLM.
Some of those tokens represent <em>meta</em> things, like start and end of a message.
Other can allow the LLM to chat with the user by providing tags for separating parts of conversation (system prompt, user messages, LLM responses).
I&rsquo;ve also seen tool calling capabilities in LLM templates, which in theory should allow the LLM to use external tools, but i haven&rsquo;t tested them yet (check out Qwen2.5 and CodeQwen2.5 for example models with those functions).</p><p>We can use <code>llama-server</code> API to tokenize some text and see how it looks after being translated. Hope you&rsquo;ve got <code>curl</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>curl -X POST -H <span style=color:#e6db74>&#34;Content-Type: application/json&#34;</span> -d <span style=color:#e6db74>&#39;{&#34;content&#34;: &#34;hello world! this is an example message!&#34;}&#39;</span> http://127.0.0.1:8080/tokenize
</span></span></code></pre></div><p>For SmolLM2, the response should be following:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{<span style=color:#f92672>&#34;tokens&#34;</span>:[<span style=color:#ae81ff>28120</span>,<span style=color:#ae81ff>905</span>,<span style=color:#ae81ff>17</span>,<span style=color:#ae81ff>451</span>,<span style=color:#ae81ff>314</span>,<span style=color:#ae81ff>1183</span>,<span style=color:#ae81ff>3714</span>,<span style=color:#ae81ff>17</span>]}
</span></span></code></pre></div><p>Which we can very roughly translate to:</p><ul><li>28120 - hello</li><li>905 - world</li><li>17 - !</li><li>451 - this</li><li>314 - is</li><li>354 - an</li><li>1183 - example</li><li>3714 - message</li><li>17 - !</li></ul><p>We can pass this JSON back to <code>/detokenize</code> endpoint to get our original text back:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>curl -X POST -H <span style=color:#e6db74>&#34;Content-Type: application/json&#34;</span> -d <span style=color:#e6db74>&#39;{&#34;tokens&#34;: [28120,905,17,451,314,354,1183,3714,17]}&#39;</span> http://127.0.0.1:8080/detokenize
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{<span style=color:#f92672>&#34;content&#34;</span>:<span style=color:#e6db74>&#34;hello world! this is an example message!&#34;</span>}
</span></span></code></pre></div></li><li><p>Dank Magick (feeding the beast)</p><p>I honestly don&rsquo;t know what exactly happens in this step, but i&rsquo;ll try my best to explain it in simple and very approximate terms.
The input is the tokenized prompt.
This prompt is fed to the LLM, and the digestion process takes a lot of processing time due to the insane amount of matrix operations that must be performed to satisfy the digital beast.
After the prompt is digested, the LLM starts talking to us.
LLM talks by generating pairs of tokens and probabilities of them appearing next in the completed text.
If we&rsquo;d just use those as-is, the output would be complete gibberish and would drive people insane, as it&rsquo;s usually the case with demons - digital or not.
Those tokens must be filtered out in order to form an output understandable to human beings (or whatever other beings you want to talk with), and that&rsquo;s what token sampling is all about.</p></li><li><p>Token sampling</p><p>This is probably the most interesting step for us, because we can control it&rsquo;s every single parameter.
As usual, i advise caution when working with raw output from demons - digital or not, it may result in unexpected stuff happening when handled incorrectly.
To generate a token, LLM outputs a batch of token-probability pairs that&rsquo;s filtered out to a single one by a chain of samplers.
There&rsquo;s plenty of different sampling algorithms that can be tweaked for different purposes, and list of those available in llama.cpp with their descriptions is presented below.</p></li><li><p>Detokenization</p><p>Generated tokens must be converted back to human-readable form, so a detokenization must take place.
This is the last step.
Hooray, we tamed the digital beast and forced it to talk.
I have previously feared the consequences this could bring upon the humanity, but here we are, 373 1457 260 970 1041 3935.</p></li></ol><h3 id=list-of-llm-configuration-options-and-samplers-available-in-llamacpp>list of LLM configuration options and samplers available in llama.cpp<a href=#list-of-llm-configuration-options-and-samplers-available-in-llamacpp class=hanchor arialabel=Anchor>#</a></h3><ul><li><strong>System Message</strong> - Usually, conversations with LLMs start with a &ldquo;system&rdquo; message that tells the LLM how to behave.
This is probably the easiest-to-use tool that can drastically change the behavior of a model.
My recommendation is to put as much useful informations and precise behavior descriptions for your application as possible, to maximize the quality of LLM output.
You may think that giving the digital demon maximum amount of knowledge may lead to bad things happening, but our reality haven&rsquo;t collapsed yet so i think we&rsquo;re good for now.</li><li><strong>Temperature</strong> - per <code>llama.cpp</code> docs &ldquo;Controls the randomness of the generated text by affecting the probability distribution of the output tokens. Higher = more random, lower = more focused&rdquo;.
I don&rsquo;t have anything to add here, it controls the &ldquo;creativity&rdquo; of an LLM.
High values result in more random and &ldquo;creative&rdquo; output, but overcooking the beast may result in hallucinations, and - in certain scenarios - screams that will destroy your sanity.
Keep it in 0.2-2.0 range for a start, and keep it positive and non-zero.</li><li><a href=https://rentry.org/dynamic_temperature><strong>Dynamic temperature</strong></a> - Dynamic temperature sampling is an addition to temperature sampler.
The linked article describes it in detail, and it&rsquo;s pretty short so i strongly recommend reading it - i can&rsquo;t really do a better job explaining it.
There&rsquo;s also the <a href=https://www.reddit.com/r/Oobabooga/comments/191klr8/some_information_about_dynamic_temperature_added/>reddit post</a> with more explanations from the algorithm&rsquo;s author.
However, in case the article goes down - the short explanation of this algorithm is: it tweaks the temperature of generated tokens based on their entropy.
Entropy here can be understood as inverse of LLMs confidence in generated tokens.
Lower entropy means that the LLM is more confident in it&rsquo;s predictions, and therefore the temperature of tokens with low entropy should also be low.
High entropy works the other way around.
Effectively, this sampling can encourage creativity while preventing hallucinations at higher temperatures.
I strongly recommend testing it out, as it&rsquo;s usually disabled by default.
It may require some additional tweaks to other samplers when enabled, to produce optimal results.
The parameters of dynamic temperature sampler are:<ul><li><strong>Dynatemp range</strong> - the range of dynamic temperature to be added/subtracted</li><li><strong>Dynatemp exponent</strong> - changing the exponent changes the dynamic temperature in following way (figure shamelessly stolen from <a href=https://rentry.org/dynamic_temperature>previously linked reentry article</a>): <img alt="dynatemp exponent effects" src=/img/llama-cpp/dynatemp-exponent.png></li></ul></li><li><strong>Top-K</strong> - Top-K sampling is a fancy name for &ldquo;keep only <code>K</code> most probable tokens&rdquo; algorithm.
Higher values can result in more diverse text, because there&rsquo;s more tokens to choose from when generating responses.</li><li><strong>Top-P</strong> - Top-P sampling, also called <em>nucleus sampling</em>, per <code>llama.cpp</code> docs &ldquo;Limits the tokens to those that together have a cumulative probability of at least <code>p</code>&rdquo;.
In human language, it means that the Top-P sampler takes a list of tokens and their probabilities as an input (note that the sum of their cumulative probabilities is by definition equal to 1), and returns tokens with highest probabilities from that list until the sum of their cumulative probabilities is greater or equal to <code>p</code>.
Or, in other words, <code>p</code> value changes the % of tokens returned by the Top-P sampler.
For example, when <code>p</code> is equal to 0.7, the sampler will return 70% of input tokens with highest probabilities.
There&rsquo;s a <a href=https://rumn.medium.com/setting-top-k-top-p-and-temperature-in-llms-3da3a8f74832>pretty good article</a> about temperature, Top-K and Top-P sampling that i&rsquo;ve found and can recommend if you wanna know more.</li><li><strong>Min-P</strong> - Min-P sampling, per <code>llama.cpp</code> docs &ldquo;Limits tokens based on the minimum probability for a token to be considered, relative to the probability of the most likely token&rdquo;.
There&rsquo;s a <a href=https://arxiv.org/pdf/2407.01082>paper</a> explaining this algorithm (it contains loads of citations for other LLM-related stuff too, good read).
Figure 1 from this paper nicely shows what each of the sampling algorithms does to probability distribution of tokens:
<img alt=min-p-probabilities src=/img/llama-cpp/min-p-probs.png></li><li><a href=https://www.reddit.com/r/LocalLLaMA/comments/1ev8n2s/exclude_top_choices_xtc_a_sampler_that_boosts/><strong>Exclude Top Choices (XTC)</strong></a> - This is a funky one, because it works a bit differently from most other samplers.
Quoting the author, <em>Instead of pruning the least likely tokens, under certain circumstances, it removes the most likely tokens from consideration</em>.
Detailed description can be found in <a href=https://github.com/oobabooga/text-generation-webui/pull/6335>the PR with implementation</a>.
I recommend reading it, because i really can&rsquo;t come up with anything better in few sentences, it&rsquo;s a really good explanation.
I can, however, steal this image from the linked PR to show you more-or-less what XTC does: <img alt=xtc src=/img/llama-cpp/xtc.png>
The parameters for XTC sampler are:<ul><li><strong>XTC threshold</strong> - probability cutoff threshold for top tokens, in (0, 1) range.</li><li><strong>XTC probability</strong> - probability of XTC sampling being applied in [0, 1] range, where 0 = XTC disabled, 1 = XTC always enabled.</li></ul></li><li><a href=https://arxiv.org/pdf/2202.00666><strong>Locally typical sampling (typical-P)</strong></a> - per <code>llama.cpp</code> docs &ldquo;Sorts and limits tokens based on the difference between log-probability and entropy&rdquo;.
I&mldr; honestly don&rsquo;t know how exactly it works.
I tried reading the linked paper, but i lack the mental capacity to understand it enough to describe it back.
<a href=https://www.reddit.com/r/LocalLLaMA/comments/153bnly/what_does_typical_p_actually_do/>Some people on Reddit</a> also have the same issue, so i recommend going there and reading the comments.
I haven&rsquo;t used that sampling much, so i can&rsquo;t really say anything about it from experience either, so - moving on.</li><li><a href=https://github.com/oobabooga/text-generation-webui/pull/5677><strong>DRY</strong></a> - This sampler is used to prevent unwanted token repetition.
Simplifying, it tries to detect repeating token sequences in generated text and reduces the probabilities of tokens that will create repetitions.
As usual, i recommend reading the linked PR for detailed explanation, and as usual i&rsquo;ve stolen a figure from it that shows what DRY does: <img alt=dry src=/img/llama-cpp/dry.png>
I&rsquo;ll also quote a short explanation of this sampler:
<em>The penalty for a token is calculated as <code>multiplier * base ^ (n - allowed_length)</code>, where <code>n</code> is the length of the sequence before that token that matches the end of the input, and <code>multiplier</code>, <code>base</code>, and <code>allowed_length</code> are configurable parameters.</em>
<em>If the length of the matching sequence is less than <code>allowed_length</code>, no penalty is applied.</em>
The parameters for DRY sampler are:<ul><li><strong>DRY multiplier</strong> - see explanation above</li><li><strong>DRY base</strong> - see explanation above</li><li><strong>DRY allowed length</strong> - see explanation above. Quoting <code>llama.cpp</code> docs: <em>Tokens that extend repetition beyond this receive exponentially increasing penalty</em>.</li><li><strong>DRY penalty last N</strong> - how many tokens should be scanned for repetition. -1 = whole context, 0 = disabled.</li><li><strong>DRY sequence breakers</strong> - characters that are used as separators for parts of sentences considered for DRY. Defaults for <code>llama.cpp</code> are <code>('\n', ':', '"', '*')</code>.</li></ul></li><li><a href="https://openreview.net/pdf?id=W1G1JZEIy5_"><strong>Mirostat</strong></a> - is a funky sampling algorithm that <strong>overrides Top-K, Top-P and Typical-P samplers</strong>.
It&rsquo;s an alternative sampler that produces text with controlled <em>perplexity</em> (entropy), which means that we can control how certain the model should be in it&rsquo;s predictions.
This comes without side-effects of generating repeated text (as it happens in low perplexity scenarios) or incoherent output (as it happens in high perplexity scenarios).
The configuration parameters for Mirostat are:<ul><li><strong>Mirostat version</strong> - 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0.</li><li><strong>Mirostat learning rate (η, eta)</strong> - specifies how fast the model converges to desired perplexity.</li><li><strong>Mirostat target entropy (τ, tau)</strong> - the desired perplexity.
Depending on the model, it should not be too high, otherwise you may degrade it&rsquo;s performance.</li></ul></li><li><strong>Max tokens</strong> - i think that&rsquo;s pretty self-explanatory. -1 makes the LLM generate until it decides it&rsquo;s end of the sentence (by returning end-of-sentence token), or the context is full.</li><li><strong>Repetition penalty</strong> - Repetition penalty algorithm (not to be mistaken with DRY) simply reduces the chance that tokens that are already in the generated text will be used again.
Usually the repetition penalty algorithm is restricted to <code>N</code> last tokens of the context.
In case of <code>llama.cpp</code> (i&rsquo;ll simplify a bit), it works like that: first, it creates a frequency map occurrences for last <code>N</code> tokens.
Then, the current logit bias for each token is divided by <code>repeat_penalty</code> value.
By default it&rsquo;s usually set to 1.0, so to enable repetition penalty it should be set to >1.
Finally, frequency and presence penalties are applied based on the frequency map.
The penalty for each token is equal to <code>(token_count * frequency_penalty) + (presence_penalty if token_count > 0)</code>.
The penalty is represented as logit bias, which can be in [-100, 100] range.
Negative values reduce the probability of token appearing in output, while positive increase it.
The configuration parameters for repetition penalty are:<ul><li><strong>Repeat last N</strong> - Amount of tokens from the end of the context to consider for repetition penalty.</li><li><strong>Repeat penalty</strong> - <code>repeat_penalty</code> argument described above, if equal to <code>1.0</code> then the repetition penalty is disabled.</li><li><strong>Presence penalty</strong> - <code>presence_penalty</code> argument from the equation above.</li><li><strong>Frequency penalty</strong> - <code>frequency_penalty</code> argument from the equation above.</li></ul></li></ul><p>Additional literature:</p><ul><li><a href=https://www.reddit.com/r/LocalLLaMA/comments/17vonjo/your_settings_are_probably_hurting_your_model_why/>Your settings are (probably) hurting your model</a></li></ul><p>In <em>Other sampler settings</em> we can find sampling queue configuration.
As i&rsquo;ve mentioned earlier, the samplers are applied in a chain.
Here, we can configure the order of their application, and select which are used.
The setting uses short names for samplers, the mapping is following:</p><ul><li><code>d</code> - DRY</li><li><code>k</code> - Top-K</li><li><code>y</code> - Typical-P</li><li><code>p</code> - Top-P</li><li><code>m</code> - Min-P</li><li><code>x</code> - Exclude Top Choices (XTC)</li><li><code>t</code> - Temperature</li></ul><p>Some samplers and settings i&rsquo;ve listed above may be missing from web UI configuration (like Mirostat), but they all can be configured via environmental variables, CLI arguments for <code>llama.cpp</code> binaries, or llama.cpp server API.</p><h2 id=final-thoughts>final thoughts<a href=#final-thoughts class=hanchor arialabel=Anchor>#</a></h2><p>That is a <strong>long</strong> post, damn.
I have started writing this post at the end of October.
It&rsquo;s almost December now.
During that time, multiple new models have been released - including SmolLM2, fun fact - i have originally planned to use Llama 3.2 3B.
The speed at which the LLM community moves and releases new stuff is absolutely incredible, but thankfully <code>llama.cpp</code> is <em>relatively</em> stable now.
I hope the knowledge i&rsquo;ve gathered in this post will be useful and inspiring to the readers, and will allow them to play with LLMs freely in their homes.
That&rsquo;s it, i&rsquo;m tired.
I&rsquo;m releasing that shit into the wild.</p><p>Suggestions for next posts are welcome, for now i intend to make some scripts for automated benchmarks w/ <code>llama-bench</code> and gather some data.
I&rsquo;ll try to keep this post up-to-date and <em>maybe</em> add some stuff if it&rsquo;s requested.
Questions are welcome too, preferably in the comments section.
Have a nice one.</p><h3 id=bonus-where-to-find-models-and-some-recommendations>bonus: where to find models, and some recommendations<a href=#bonus-where-to-find-models-and-some-recommendations class=hanchor arialabel=Anchor>#</a></h3><p>My favorite site for finding models and comparing them is <a href=https://llm.extractum.io>LLM Explorer</a>.
It&rsquo;s basically a search engine for models.
The UI is not exactly <em>great</em> but it&rsquo;s <em>good enough</em>, it has the most comprehensive list of LLMs i&rsquo;ve seen, and lots of search options.</p><p>As for my recommendations, some relatively recent models i&rsquo;ve tried that made a positive impression upon me are:</p><ul><li>Google Gemma 2 9B SimPO - a fine-tune of Google Gemma model. Gemma models are pretty interesting, and their responses are noticeably different from other models.</li><li>Meta Llama 3.1/3.2 - i recommend trying out Llama 3.1 8B Instruct, as it&rsquo;s the default go-to model for most LLM applications. There&rsquo;s also many finetunes and <em>abliterated</em> versions that don&rsquo;t have any built-in restrictions available publicly.</li><li>Microsoft Phi 3.5 - a series of models from Microsoft. Most of them are small, but there&rsquo;s also big MoE (Mixture of Experts) version available.</li><li>Qwen/CodeQwen 2.5 - series of models from Alibaba, currently one of the best open-source models available. At the moment of writing this, CodeQwen 14B is my daily driver model.</li></ul></div></div><div class=pagination><div class=pagination__title><span class=pagination__title-h>Read other posts</span><hr></div><div class=pagination__buttons><a href=https://steelph0enix.github.io/posts/making-c-cpp-project-template-in-meson-part-2-5/ class="button inline next">Making C/C++ project template in Meson - part 2.5</a></div></div><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//steelph0enixblog.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></article></div><footer class=footer><div class=footer__inner><div class="copyright copyright--user"><span>SteelPh0enix's Blog © 2021-2024 by SteelPh0enix is licensed under CC BY-SA 4.0</span>
<span>:: <a href=https://github.com/panr/hugo-theme-terminal target=_blank>Theme</a> made by <a href=https://github.com/panr target=_blank>panr</a></span></div></div></footer><script type=text/javascript src=/bundle.min.js></script></div></body></html>